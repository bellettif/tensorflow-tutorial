{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - End-to-end training for computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a quick recap on Stochastic Gradient Descent in tutorial 1, tutorial 2 showed through two very simple regression examples how using an automated differentiator such as Tensorflow made coding more seamless.\n",
    "\n",
    "We wrote down how to compute our expected loss (going back to the principle of rationality that we started the class with) and attempted to minimize it by correcting errors as we notice them.\n",
    "\n",
    "Using an automated differentiator for linear or logistic regression problems is a bit of an overkill, but now we are going to use this tool up for its full potential with complex classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying images\n",
    "\n",
    "Image classification is one of the most challenging tasks machine learning engineers attempt to achieve. Lately, the use of deep convolutional networks have achieved a significant breakthrough in the field.\n",
    "\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "\n",
    "We are going to focus on a less challenging task and will attempt to classify hand written digits in this tutorial using the MNIST dataset.\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "Let's load the dataset (this may take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "examples_to_show = 10\n",
    "f, a = plt.subplots(1, examples_to_show, figsize=(10, 2))\n",
    "\n",
    "for i in range(examples_to_show):\n",
    "    a[i].imshow(np.reshape(mnist.train.images[i], (28, 28)))\n",
    "    a[i].set_title('Label=%d' % mnist.train.labels[i])\n",
    "    a[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making room for the data\n",
    "\n",
    "The mnist images are (28 x 28 x 1) in size, but represented as a flat vector, let's prepare a placeholder for that.\n",
    "\n",
    "Let's also prepare a placeholder for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create a placeholder for the variable X\n",
    "#\n",
    "#    Your code here\n",
    "#\n",
    "\n",
    "X =\n",
    "\n",
    "## Create a placeholder for the variable Y\n",
    "#\n",
    "#    Your code here\n",
    "#\n",
    "Y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the images\n",
    "\n",
    "We want to transform X from a collection of vectors to a collection of black and white images. Such an image has a shape (28, 28, 1), using the reshape operator, change the shape of X accordingly.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#    Your code here\n",
    "#\n",
    "\n",
    "X_IMAGE = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a convolution filter\n",
    "\n",
    "Let's create 32 filters of size (4, 4) that we will apply to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIRST_FILTERS = tf.Variable(tf.truncated_normal([5, 5, 1, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the filters to the image.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_CONVED = tf.nn.conv2d(X_IMAGE,\n",
    "                        FIRST_FILTERS,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply a non-linear activation to this filter bank. We choose the linear rectified activation which has the advantage of preventing exploding or vanishing gradients in the back-propagation.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FIRST_BIASES = tf.Variable(tf.truncated_normal([32]))\n",
    "                           \n",
    "FIRST_ACTIVATIONS = tf.nn.relu(X_CONVED + FIRST_BIASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to coalesce the activations, we use a max pooling layer.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_LAYER_1 = tf.nn.max_pool(FIRST_ACTIVATIONS, \n",
    "                                ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just transformed a shallow (only one channel) but wide image into a representation that is more compact but richer as it features many more channels (16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Shape of input = ', X.get_shape())\n",
    "print('Shape of output = ', OUTPUT_LAYER_1.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE\n",
    "\n",
    "Repeat the same operation with the same choice of strides and pooling size but with filter banks of size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "\n",
    "#FILTERS_2 =\n",
    "\n",
    "#BIASES_2 = \n",
    "\n",
    "#ACTIVATIONS_2 = \n",
    "\n",
    "#OUTPUT_LAYER_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layer:\n",
    "\n",
    "The transformations we were using so far were very structured (convolutions). We can in fact multiply the output of a layer by a matrix which is called a fully connected layer.\n",
    "\n",
    "The first step is to reshape the output of the third layer so as to flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_LAYER_2 = tf.reshape(OUTPUT_LAYER_2, (-1, 3136))\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([3136, 1024]))\n",
    "BIAS = tf.Variable(tf.truncated_normal([1024]))\n",
    "\n",
    "OUTPUT_LAYER_3 = tf.nn.relu(tf.matmul(OUTPUT_LAYER_2, W) + BIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has a lot of parameters, to prevent overfitting we use a regularization technique called dropout which randomly reactivates activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DROPOUT_RATE = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "OUTPUT_LAYER_3 = tf.nn.dropout(OUTPUT_LAYER_3, 1.0 - DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a logit unit that translates an input of dimension 1024 into 10 different intensities corresponding to 10 labels.\n",
    "As this layer involves a full unstructured matrix multiplication it is often called a fully connected layer.\n",
    "This layer involves a large number of parameters compared to convolution layers and therefore they are used parsimoniously.\n",
    "Feel free to add a non linearity at the end of this layer and add another fully connected layer on top if you want to change the architecture of this neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "\n",
    "LOGIT = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the corresponding loss and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOSS = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=LOGIT))\n",
    "ACCURACY = tf.reduce_mean(tf.cast(tf.nn.in_top_k(LOGIT, Y, 1), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "A particular perk of modern automated differientators is that they make different optimization algorithm available for us.\n",
    "\n",
    "For now, let's use a stochatic gradient descent routine with a varying step size.\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.11/api_docs/python/train/decaying_the_learning_rate\n",
    "https://www.tensorflow.org/versions/master/api_docs/python/train/decaying_the_learning_rate#inverse_time_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-3\n",
    "learning_rate = tf.train.inverse_time_decay(starter_learning_rate, \n",
    "                                            global_step,\n",
    "                                            1, 1e-5)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(LOSS, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each time we run the optimizer, the stochastic gradient descent update for the variables is automatically computed.\n",
    "\n",
    "We iterate multiple times over the data set, once we have seen the entire dataset, an epoch has elapsed. This will be a bit slow over most machines which (like my laptop) do not have a GPU.\n",
    "\n",
    "Here we do not have a very optimize neural network, however, it is now very easy to play with the number of convolution filters, add a fully connected layer etc to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train = mnist.train.num_examples\n",
    "n_test = mnist.test.num_examples\n",
    "batch_size = 32\n",
    "n_epochs = 100\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for step in range(n_train // batch_size):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={X: batch_x, Y: batch_y, DROPOUT_RATE:0.8})\n",
    "    x_train, y_train = mnist.train.next_batch(n_train)[:n_test]\n",
    "    train_loss, train_accuracy = sess.run((LOSS, ACCURACY), \n",
    "                                          feed_dict={X: x_train, Y: y_train, DROPOUT_RATE:0.0})\n",
    "    x_test, y_test = mnist.test.next_batch(n_test)\n",
    "    test_loss, test_accuracy = sess.run((LOSS, ACCURACY), \n",
    "                                        feed_dict={X: x_test, Y: y_test, DROPOUT_RATE:0.0})\n",
    "    \n",
    "    print('Epoch %d, train_acc=%.2f, test_acc=%.2f' % (epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "Change the optimizer to an adaptive learning rate optimizer such as RMSProp or ADAM. They greatly help with problems that are ill-conditioned (where the slope of the gradient for one parameter is much bigger than for another).\n",
    "\n",
    "https://www.tensorflow.org/api_guides/python/train\n",
    "\n",
    "Experiment with gradient clipping and batch normalization to prevent numerical issues and accelerate training.\n",
    "\n",
    "Visualize your training process and your graph of tensors with tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
