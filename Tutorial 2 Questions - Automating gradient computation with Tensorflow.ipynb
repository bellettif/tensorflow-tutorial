{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating gradient computation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a somewhat more involved machinery to automate gradient computation!\n",
    "\n",
    "First of all, to get a short introduction that will be used for automated differentiation in this tutorial, visit:\n",
    "https://www.tensorflow.org/get_started/get_started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATMAAAEACAYAAADftpFdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsvXeYZFd55/99z7m3qjrPdE/uyTPKmhlJSAgJISORZEQG\nY2PjiO0F2xindeLnNcbeXfuB/TnCsvYaGwMCLESSwWQJCSGhgHIaaTQ5d06V7jnv/nFurLrdXd1T\nVV1dfT561N1T4d5bVbe+982HmBkWi8Wy3BFLfQAWi8VSD6yYWSyWtsCKmcViaQusmFkslrbAipnF\nYmkLrJhZLJa2wIqZxWJpC6yYWSyWtsCKmcViaQusmFkslrbAipnFYmkLnCXct20KtVgsadBinmQt\nM4vF0hZYMbNYLG2BFTOLxdIWLGXMzNJicPIHAAItKnphsTQfK2YrnGA4Z/qQTkY8FktW2QAAnBD8\n2bAXgmZjxWyFYr6PSSELf6MinUS0uPRSGxIXstmmNBvRZzDbC0AzsWK2Aol/CZnZ/A+ANSP8osYs\nCwIAomqRW5FEQsYMcIWFFn9/iAjMbAWtSVgxWyGkWRSBkGlmIPYbAEAEQQJEgPDTRIGorXQCIdOs\nAf89DCAi/72LhMxYaLAWboOxYrYC4PjPwKJgDc0M1mz+1oGFxjDfOwEIhhAy4S6tZOssboMx6/C9\nCy4I5n0jEAkwEUgY8QreLwq2YUWtIVgxa3MSVpj5A1pHQqa1Svwb8L+QggFIEBl5C92lFa5moVvO\nDM3avHfat9AQiJmGIAHBAkJQYJYB/v2RqWapJ1bM2pBAtMLfvqDpmBUWfAm1VlBahxYbAAgiCAjf\nVSKAyX75AMRtM2YTY9QqErRA50kYF12I5P9EDPJd9zg2plYfrJi1GYkMWywmxtpYEqw1lNbhl1D5\nllkgZkRkXCQiaMEQiMJo/kaxck0zQxB/NO+Z/x4qHb73CTGTApIlmBlCEEgAAlEyxSYJ6ocVszYi\nEJ3IEvPjOaElpqA1QynlC1l0W/AcQQRyJDQzRJjcrMzZrVyC+KPmmJUbs84AAJogSBsxYwGWDMEC\nkgUEMyCEiUkG2WJf0Gxt2rlhxWyZw3GxiZULsC9QSitjjSnt/23ETGljpYXxssA6EAKkNYQI4kBW\nxhIkDd8wfqYCVzN2URBCQGgB1gwhBFhKCMkQzBDkW2oU/R9ZvVbUFoMVs2VKZdErI/picSwmFriR\nWlX+HVhtMfco+FL5FhmbH5YEnPjlXzrCxEr4GRD51q32XUxhfrNxP6UQYA5iaSYDKkRQxmELbheD\nFbNlSGWAX6eImGZt3MkgNuZbYzpmQQT/J7YdCGPqTi1J/KQJw6/RQ+hqKpMhgNDCxB6FgNYaQktI\nIaClETSTHJAQgsFsrDmEBR3W9VwIVsyWEVVtR/EyAR1kKRVUPLivdELE4kKmYxX/ptgz+GeFwDXv\nJS5Loi4KHbr44X0U1KJJaK0hWUMLAakltAysNIbw/w7KO4JMMlEs4WLr0+bEitkyIK3UIix81clS\ni0DAqqyxsMgz9n8iAESgyqylVbE5qXx7Aist2S7mO6IcBfqFYGjBkNoXMBlzP6WEIAGIII6WrE8L\nxM5SjRWzFietjxLMxvryBUr51hjHBCwswaiIjZkvl67+JtrvR13g8BoQvMEMaIKC9mNiDNYaLP1y\nDT8BI7Up4wAzWEhoJkg/6xm1lBmX09bcpmPFrAXhmKtX1Ufpi5JxJaNSC+X/rZXyK9N51vhYuo4F\n7ow1x86NqL+V47eBoTWFdXzaTwqY8g0GB2UczMbtFGSyn+QnD+C7niDEW/6tlRZhxazFqC61iOJb\nUWxMhwIWL7WIxMtYZSYgzQlLzEpVc+C0vwM30c9Ympv8C1QgZpqhJZvkAHOU9WTh16fFuzFskiCO\nFbMWoarUIqwXC5rAdRjYj4tZYI1Vllok4mJV4a/4v+y3oFkYIQtilRRmPk0G089KC1OHpgVBamms\nM6EhpIT0awGFkDCaZtqjgqzqSrfSrJgtMVFrTPA7rY/Sdydj2cmgij+0xmLFr4GIVbchWZpP5WXE\nF5xYobK5ABEEsV+LxmFiQPttUDJwRYWAaSIwZRxKa4gKEVupombFbAlJ1nhV91EGBa5pbUhKqcjt\nDDOV0XYq9tS8F2WZB2OVJV1PBrHfExtekEw5hwjiZ5pjVhqH/Z4khCnZsK6nFbOlorJmLNlHGWQk\nk8H9QMyCDGZlvdj8LqWlNYgC+EEMzbelISD8CxQgNaCF6fFk4VtpRJCaTTmHEJCSw1gaCRHNTvOz\nniupNs2KWROJBAwILKhwammisFWlTLaI3M248IXbSe6p6a/NslDiuU4Kb9Ksw7YyRcpYbBx3Pym0\n4KOWKGncTuZYW1Tl/Ln2t9SsmDWBylKLqOi1cizP/KUWieB+aqlF/YWszb8DLUDwmVEiix2IWhBO\nCEo5TMGthpYSWgsIqf3WKNPIbjoHyO/7RNhJ0O5JAitmDSYUHYoVTALRlFfmsAlcecqvIavuo6yq\n4E/upY5HHIu9tO9536IEF6fqJEFc1KQUYC1Mwa2UkFqYFqmgk4CEeQyi1ihjmVF4AWxHUbNi1iCq\n+ij9bCPitWJxVzIUsMiVTKveD2vQoj3V6Yhplr9nv8nSKFJEDYBfhAEoQBNDBgkjaUYNaWYzM02I\nWPIgaI0KJt22bxO7FbM6k170WtlHmTIkcY6pFppNPVLD42Kx1Tf8a7kVsRYgqE/TvpUWb40SQTyN\ntPk73u/pi5yURtiCBEGwyhYF5TttkiSwYlZHOOYWhC4h/EmkQWysakii/3esIVxrjoSwYRlKin76\nReUCwaDAyA2hSNYSz7M0g+okgTm/Yq1RQUkPUdQWFZ+dRgIMY7GRZuN+EoEUhcMhw86EZW6mWTE7\nRyqD+wAS7uFcpRYL76NsgEtJ/qjsILUfm3wajAVa5ud4mxBLEsQKrYkZENFEjsBaC5vXfWFjGXc/\nyZ+vFp9y67uey9hKs2J2DqRV70d9lDqaXhGb8hpvSYoPUoRfntH4WrGYRQb4C/0mBSxxW2LlR1jD\nbMmJF92an0qr8DOLZz7jVpoRMe2P7Rb+6G5/0m20HEEUTVuGVzArZosgbSxPGBfzM5bBVIsgyF8p\nZpV9lEFyoNGWWOhWIilgUpoJqFJKE2ORwh8YSOFq3MktWZaO+PlnLjfx+rQobxDF1DSbtTylZkgp\nwUKDhT+GyJ9wyzAdCEkhWz5JAitmC6CyXSgcMV21HqWKrLG0Sa9N66OsjnQFUxcC11II4V+hCdKR\nkDK21mPcYqvTEVnqT6I1KkgSsKpyPc1o7li/pzAxNM3Cb2L3Fy32CRZZWS71aVbMaqSy3zExd7+i\nj1KFkywia8w8ToW9l2ZzukLEgEYVvSZjYNWL1MatstA6i8XQzIZa/4ReeSQLbgnRRZbCcyw2C08I\nCI63SElI30oTwm+oChdY8bdMwUIrS/H6aseK2RxUupPB70jIUhbUjf2t/IVhtdLQwUkVbwiPtl7H\no67IUibcSYRXX0ECQkr/Ci3NiBlhii1DIQtX4rYuZusTj6IFjZkMpYP1BIL2psAy80d3sz9uyLfS\nmKVJFvidA0JQLEoXFN8u1WucGytms1BV9Bqv+9JpwX2uqhULJ1oww8T4GxUXm9udNBnLpPsopDAi\nFqy67f8tpYgELyZkVsqWE7EkQaw+LVw9Pd4a5f+thYbWElKaFaTCBVbYdz2XQRzNilkFlaUWlX2U\n6WtQVpRaKA5FLN4MHm053Ftdjz2wxgQZ/4BE3J0MrDERWmXhUmf+smehJQZKxE5a9eS1zEVyMgc4\nGDVkOgGC+jTWZrUolqaDQOtYW5QwF7lwfU/TTGC2ya13Tlgx86m2xEx2MtFH6deNKaWS1fuVAX5w\nE/oo0zKUIix4DayqYMUfIgrjYiTM38JfCchYYTLhikZhshY7Yy0LoKLoNoyh+XPTBINJhDPUjOsZ\ntEWZUg5IBkOawltIcBhHQ8slBqyYId2l1DGXMiq1qOij9KdcBBbb3H2UjZtmUVUjJox7WelCBm5k\nPPAfD/KHsbEVMjJmZVFRn8YAawCko6ynPyctGA4phAQYkAxABhdNCV/PWkrIgBUuZulFr2l9lEFs\nLBKztEV1m9dHSQkhi6ywIAsVrZYdBPPj7mQQ3A8nKvilF/GsZWudppb6ELme5p9m2eLQ9Yy7n2xW\niTLP8c/f4CInZUte6FacmAXxA/MrHhvDrH2UUWwsErOwXcmv3q92KRvQCB53LQkA/KJWPx5mFpCl\nioC+jOJkoZBRJGShBWYtsZVBZdYTYGhQOPOsel4e/McGlr8Z6R35m61y2qw4MasUsuRYnqiPsjLQ\nb8os/DKMiur99OB+4zAnT5RxDC2xWBV/IGCRZeaPgRFJK2y5CBmrEnjkfkAXQf1Xgty+pT6kNiCe\nJIh66IJVowBA+U3pmiJPJCrXQJWxt5SsGDELMzqVfZTx9iOOuZChJRZ3KbnJfZTVBEKUEDIp/fow\nJ6wVC4P7MiqCTEzDqPjdynD+OPSBfwIP3QOwB1p9OcT2nwet2rfUh9Zg5vps6nWeVWQ9/RM6EDQi\ngiaKSoyC71BYRds6arYixIyrRAyx+i9d3XpUkamsdCmjBUTCPTThVVA0hyrWihQImSMd044UWmgy\nTASEc6yWU0yMNdibAp+4HfrAPwE6H901dA/U0D0QW98B2vKTQG4diOQSHmw9qW3YElfdey7nYLLX\ns6o0CUiMtKpuv2sN2l7MElcTRBMtEmUVqmLSq4rEK23ufjMtsUoiy0xUWGZGwAJxCwpfqy2w1hcy\nLo2Dz94JPnE7ePwJzPYe6yOfBYZ/CLHpZmDdjaCOTc090AZC4Y/KOSfxVqX4O0NoxLkYjnqXZvpG\nlA/gFrLJDFTZc9hEGr7j6vli8ZHVUeuRp1SyNSlNxKpcyma/byauFQTyjSUm4DguHEfCkQ4cR1aV\nW0RWWOvHxcAaPPJDqAP/F5h6HtCF2p5HDtC9G2L7z4HW3QAiMf9zFnN4sVip5ymUvTK8sodyuYyy\n5yXqDxd3nlDiZ1D3F7sxsbn0eO25npfRMQghIQVBOg5cx4HrunBcB67j+iU+MnHBrCOL2mDbWmaV\n1lQ0plqFi+hWxcf8AH9aRmfpRAxIfLZB3Cvos/SLYWetG1sGLiXrMlAeg37uI+DT3wa4vMANeMDk\nM9CP/zFo7csgzvtNILcBJDKNOeCGECVkKsczUeyCxJUuoO91UDi0gOp4rnLozrK/z8Aqi7bfOmdX\n24pZAPsxrkDIPE/5TeAqmi2mIzGbvY9y6YMEFPxHiIpiEw3hKZNiWxyePgQ+/W3oE18GCmfOfXtn\n74Yafwq08cchNr0O1LWjDkfZaOJxsmDqb3JIpggmKPolRIlJGMx+qUTQshQrvQDQCuduM2hLMQuu\nIvEPPHAnPaWSVlm8IVybZze71GLhUCIRQDAnf2S1LQMR82bAJ75i4l7Fs8a6qhelYfCRW6BOfwdi\n8I2gLT8Bcrrrt/0GEXcvKwcDmJKaqIxCaxPLik+XDc93sGlRWsoXswS0pZghJkbRsESzLqWqFLOm\nrUd5biSmYSBwRxCWW/jlr+FjW9UqY28GmHwW6sDHgLFH0bD3mDVQOAl94GOgM3dCXPj7QNd2kNPV\nmP2dI2F/bfB5BtlqEZXXBELHfl9lfP0IIgrrH4Ml6CjIPnL8XGidc7retKeY+QSBe82xiRdaVdWO\nGUustVzKuQgXoAissKjTvGWtMtYeMP4E9ImvgM/cAaiZ5u178hmoB98NWvdyiC1vA/Xtbcn3KR4r\nCyedxKabBNZZMMggOqcJWplaME1+6VDgacRiXO3ueradmIWhSb+yH0EDrfZFTXOyORyVLmUrf8hU\n9a/wf4r+bjW4NAr9/EfBQ3cDpTEsyXvMJfDpb0GNPggaeAnEeb8FyrRgF4FvmZmYaKyjwxe0ABZB\niRGbzDyZuC+R/78mMCXdz2QMuDGlHEtJ24lZnDB2Bg77Lc2iI75r6T+o1SNkaVTL2lz/Xhq4NAYe\nuhv6uX8AymNLfTgAGCiNgE9+DWrkIYjzfg00cE1LtUYF1lkYEw0ttGitBsAvE2EBoTWEJijSEDoq\n+lZExv1Esno/KPwGKgtvl9f5n0b7iRmn/NMv9Au6MGLm27IUslaHvSnw0A+gT/wHMPogwGqpD6ma\n4mnoJ/8ctPpFoME3gNa8FCQ7lvigkjFPQbFRTmTa1IK4GcAQDGjSIGH+11pDKG36KWPJLdIUWmjQ\nQc4TFUmC1rgAngvtJ2ZxQhGLfWgcFdPWl2b00bU+euog+NkPgSeeAlR+/icsJeyBR34IHn8ctGof\nxEXvB+XWLvVRJcsK/RhCWJxKUVYzGKQpWECTMoMVSZuyDpW00ogIWisAIpnsavnMfe20t5hVEe9B\nq8f2FtNH1w6nTRJmDeSPg49/Efrwp5f6cBaOmgEP3wv1/ddB7P510IZXg3IblvqofGI56rDmLIqO\nBsJE5ICCtTMVQYuYlaYUNBEUEYRIJr6CPuOqL8QyNNRWhphVVS2fKxUiFozRSd11vIgxeHAdjqFF\nTjYuDpl6sVPfBKYPLfXhnDP6+f8NnLkDYuPNoPWvAGVWN3HvQXV//JZZzluKXFLjepohi4IImoSf\n2YwWLlFahdZakCRgzea3L2ocNV6iaiX7ZcDKELO6kmz7Ta4rGUla0MsZpNKT5nwdBG2JDTxmBT71\nTehDnwBmjrRmXGxRaGDiKejJ54CTX4XY/RsQ/S9q7iFwrOjbz74LAURjd2IyU1FiYlxRs0q5JgGt\nlRE0JaCEEbRwWrLgMJ5GxLHC8WjTLVjBMitWzGqi2hKL1wSBYisi+T/DlpMg86AjgYu2tPxcTlZ5\nYPog9IH/Ax6+b6kPp3Fw2Yjaj34NvPmtEINvBjq3AWjcqKGg2jE4dxKr3rOpJxOULOyOmtGjglrp\nF9AKCpaYM+6n0Mb9VMrE1bTSofvJ2kyb1RRlO+OjO5aDplkxm5PqmFhl72PY2A0TjA0emGilggYE\ngKBdqr6H1jR4cj/08S+CT393SUotuKygJwCx2hSQNm2/x26DGvoBaMNrgI2vB7L1j6eZ7KJpXOBg\nfHU44cWUXxCRud+vN0vr8iAyjebC3yYLf8Ut4YuZNplPoQQUKT/bqcPfgaUWIGIX7FbtKgmwYlYD\nwQUqaPYVJMJVkIIl3JI9kWaSAWttnqthBI3qmEltolHHpVHow58Gn/gKUJ5o7s7h11SNFaCGZuA9\nlYOzw4V7iQuSTfxyFU6CD38SOPk1YPsvAwM3ov5XFN828y+CQfAeMesrgIRAVEKbHO9kbP7oNvKX\nk9MUjE7X0GTcT6WUEbMgSRCUfvhVAMHwgtBCa2E9s2JWxdwuZdhm4s9yChqC4+N2QquMgvgZAOXX\nBBFVVGK3KKzB5XHw6CPQB/35Ys0+BKXBRQV1dho8bcYCcYFRfroEdcKDuycDuVYCbpOsBlZA8Qzw\n7F9BrL4DctM74Llbce5fIzNKJzEgQbOp6icClIq61iDBbGrMIIIpwmZUT7wnN3I9g1CbWSvTNKH7\n57PvbpJS4EDYhILWIhqbDYT1bok2uvDe1sGKGYDZ6unT3MnE+pOVKx7510QzdkiHJn84C2qZxMiZ\ntT/p9T/Aw/djwfPF6nEM+TLUSB56sgToaunX4xrFewuQGx04uxw4G5p5KiuI0XvhTjwBrL4OetWr\nUM7srMuWzUQMbUJzuuLM9PNIQjCkBJgFBCO2+nwQh62w1IjC3IEQvrhB+nE0PwOq2Rc2Cvs9zeMQ\ndiIkhkWCWk3LrJhVEn54/qcWX9konGDgL1sftphIkSjMMHU//vP9wC2AmLneuqUZPH3I9FGO/BBQ\nNU56ref+lYY6Mw09XkwVsQQKUMc8qFMevEEHmX0ZiI7GTJlNg9Qk3KGvwxm5C07/TZhc9QYoLLaL\nwIgQGGDym8WFBun4Q4zVJoQpfJVS+pacucginKyB0FILjzUezPcTBSQE2I+FCWHiZYGwKX/OX/D4\nYMZa6Ha2npatRDFLt8LMP2LDDysssfiSbsFSblJUz9oPrDLzN4O0AJGCEATtXx0ZdO7zpurpp7IC\nF85An/xP8JHPAN5EHTde4yGUFXimDO/MNFDW8z8hjgeowx4KZxQyezOQGySQbc5wSgKD9DQ6h26D\nO/kQJvrfjnz2AiixmFFDHJ0b7OeLhCmXYIZZbRwMoaMq/nBxZzbxLSFNfCyK4VZbacHvqOBWQPuu\npyLy+z1FOIghCLNEya7WnF68AsUModvPcbMcCK8+wYTPxDypmDUWrIAkEkIWiFm8y0D78QYBDd2S\npwCXxs0KSGe+DUw83fz9a4aeLEKPFsD58jmJNOcZxR8WIddLODscyK1O0zJwBCBTPISBU3+DfOc+\nTPbciHzHHmARq0YFETRmhlYECCRWGxeCk2LGAiykuR0cLi0YG7KR/j74MV4QQTCD/eeZYlsNrUWY\new8GgIogO9qCcbOVIWaVmaAwZOr/KxEbC4L7RqiCFY+EIP+3DB8TxhCCDzZ0L81JQDpKa6engc7B\n5azDOcRjj0E98yFg+gXUddJrrfsvelCnpqBnzk3EKlGnFdRZBXnEQ2ZvFqKvia4nl9Ax/QByM49h\nputFGF7zy2CxENczSgYQIxzjE7ekmDXYt85ICEgtwDISNrOyPYMRjA2i2LlJ4WlHCF2F8OLOQoNI\ngtlkPePJqngGP0qV1vHNO0dWhpiFRI4+CQJx4PvHRIySA/HiS7dJGc3cj3rkkBAzc4IhJmSR1VZX\nFvvlZw88sR/6yC3g09+q6yHVfAhFD3qiCDWcnz8utlg0oE4o5IfzcM934Wx1ILqbI2oEgLiI7qkf\noCP/BMb73oDp7pdAOQM1biHoFomVSPhWmJmQTBDCFNUKIcBC+CuNC0gtwTpwP03GIIqn+b8YCW8i\nyniapIIp9iYIpopSosAqi9ZhbSEtWzliFnx0gfvImgG/xpVEMkMZxMRC1zKWsRQxKw5Imu9sbohi\nFoH7mnIsS1GawVMvQB/7ghmSWDjV/P2XFfR4EXq8AC42KbVbZJSfKEEd8+DscOFsc0CZ5n0FpZrA\n6pHPoHPqfoysegdK3Rcu4Nl+UsDHWGgAQ4C1ShRmB6EPHXM/JUtAmjNNCAkgSkwlt5zMeMK/j/1z\nOUmlm9k6tL+YUVQnZgpeCSBhpgrELCwTC5OhNSZCYau0xFJiBoTw0kZ+7CG4K3YYS4o+dhv0C/8E\nlEaXZv9TRajT080TsTgM6FGN0lgR3qEystfkmmalAQAXihAHfwh352ULFDMgYaWF/9SA33oUZM5N\n7RlD+rUX4VTaIMRBBEkifHxq0CO00CJPozo8Ei/GXeqzOkl7i5kvZMGHaVLagICGgDQWWZCx9N3J\npDUWBFIrLTFK+YwpWQwbT1034jOvYZtcngSPPgh98OPA5P4GHMQ8+9cMLnhQI3nwRLHp+68+IECP\naOS/OgPnfBfuTgfUKxrypWRmoKSMOz2SB1TKmJ2FbdH/mXQ9g6SVyXhGc8pkmNFEeCHWRJAUuY6z\ntUPF/07rWGk1EQtoPzGL+XBB3J98i4xJ+JkhX9woXgQbuZZRljIohq38kGs7jIYyz/dCD30ffOKr\nphm8iYuHBAQipqdKgLfAUosm4O0vQ51ScLZIODtdiM76WWqsNPRooUHudOQgmslWZp1XQaZpPFHw\noxSIjAWnhfBLPmKJgBpoVeFKo/3ErAIKSiOEhuDYVZgQll5EwX0ZZjRT42I1BjyX8uPn0ij0c38P\nnPk2oEvN379m6NE81NCMsUZaGJ7QKD+p4R3ykLkiC2fTuX8d9FQJ6tQUuNRIdzopaFGbkvDnGZh9\nExGUUpBSmPUvSIBF4pkNPMbm03ZiFhhmYbmFXx8jWYYlGfDvl8Fq4BT0W4oUEUtxKeehKV/hxDFp\noHAWPPQ98PHPA4WTzTiCCGawYuiZMvRIHjzT/Panc4GnGcV7ClBbHDi7XTOVYyFN7MzQBQ96rAA9\nVmjSCRCrZ/RdT5AGs4DWAJFfwsHCnyrrPyc1DtYetJ2YxYn3VQKmHCO4IJlyjKg41uQFRNKlbLHU\nc4LgXNYliFPfBc5+HZh4Ek36JiXQ06aPkqdLS5OmrQca8A57UGcVnM0OnAtqcz25pKBG89ATxYV3\nLtSNqBVKgyEoHj+LJnGwP06v/WwyQ1uLGYI2DJjYF7NGaGlRUOmPc3IplxI58yyyR/4WNHMIhOZ/\nkdjTpui1FYL7dYJnGOX9ZZSfLyNzeRbOTmfW2Wl6qgTv+CSgWiEmGEiUae6MCmxjVllwpeHAe1mq\nY20MbSlmUdCSIQSZymYdXY/i43riQrYYl3IpcAsvoHPiTmTH7oRQk83dOTO4rE2pxXB+Ca2RBqOB\n0o+KUMc9uBdmIPoFyDXnEefLUMMz4KnWcqfDroH4DZF+xR7UfkIGtKmYBSQykAJIiFnlY5aBJRbQ\n8Z3/H9n1RyG6mlcrBSAckqjHCuB889ufAEBs3g597FBzdsaAOqWghvKQmx24OyW4lIeeLrdkhjaC\nw5/pXj8n68nahOZ+G5pKELw3IhUP7qf+v9SHuwDUU2eQ/1Ye3iGvQWuAVsMFD+XnR6FOTi2ZkAFA\n5y9/ELm3/1fAyTRvpx6gDnko/WjGjCZqaSGbnURjUpsJGdDGlln0WQU1NclG22X/URYZxfsLkMck\nnN0u5IAEuXV+VUGWbtzP0rXCd9jNIvuqX4Cz58dQ/MbH4T16J3hyZKmPqvWpuObFZ+61C20rZnEo\nqJ5tNxhQxxXUkIazScK9KAPRUx9jm8vKFL0uaZZuduSGHeh4x/vhXfXjKN/5OZQf/vZSH5JliVkR\nYtb2FBneQQ/eQQ/u3gzc813TRL+IKy8zmyGJxyZavuiVMjm4F18L9+JrUXrwGyh87i/BY2eW+rCW\nBe1mlQFWzNqO8mMlqBMK7gUu5DpZ84QI1gyeLkGN5lsuS1cLmStfA2f7pSh+6xMoP3YnePjEOfZC\nthLxz7A+r8m6mZZlgR5SKI4qyPUS7gUZiLVzN1PrmTLUyIxZAanFrbG5EGsGkXv778O9+maUv/8F\nlO75AqAgH0AVAAAgAElEQVRbz0WundTZFrG/F/9ZtZuQAVbM2hdlhhOqE3m4F7lwL8oATnK8CwNQ\nZ2egh2eWb+V+BSQdODv3wdm5D+41b0T+E38Cffowls8LpJS/0uFziAO3o2XWxqUZloDy02Xkv5mH\nd9CDzmtTuT9WgPfCKPRQ+whZJc55V6Drj25B9s2/CbFx11IfzoIIc1azlBIFGXmqfE6t228zIQOs\nZbZi4CmN0sNFiBcEnF1FcMFrvogF+2vi90h09SF707vgXvYKlO//D5S++xlwvsldEwC0yEJ1rJvn\nURT9jHWohHP5gGhQApO/wAn8FQOCTdQ23sdaZpbljQfoYW2KXpsoZMwAFwne4Qy42PwvEAkJuWkX\ncm96H7r+6DMQm85DYumiBsIQKPeeh6Hr/gGFjdfNdZTmZ2yYYrTEoVmTQgoZ/h/dF61lSRQac7Et\nIuUf7Ym1zCwNgxlAgaDHJPSoAygC1i9tplRu3IGeP/sSit/7HMr3fgXqwCMN2Q+ThNezAzNbbsLM\n4CvBme45Hh2Pk0UL7gYDEoQ/zcVfAAAMbS4QmqFZx5rJo3a9WH14yl6sm2mx1Ax7gB5yoMclUKp9\nsmmzyFz/E3AvfRlKD3wdpTtuAY/UbwYciwymdv80ZjbdANU9WPPzAiczGCgaX3w6FCiQP6eMfTEz\nK5DHF542LqrwR8bPcoxt6GZaMbPUFdYAT0mo4y7gte6XhUiABjYhd9MvIXPVTch/6oPwnvkh4C1+\nOi+Tg9KayzF+2e/By64Jbq3tePwfyTVc/XHu/loVwe2atakLFAyltRmZ7S83l5iKQbOvPd5uQgZY\nMbPUCdYATwvoYQc8JQBePl8WMbAJnb/2d/CevAelOz4N7/mHgVKh5uczSXgDe5Df/gYU1l8LRe6i\njoNiK3+Fa1MIEa4cFiQDJCRYa2jNEFpD+/+rYAESjuJuUSwt6Xday8xiSYELBHXGNSKmlucXhNwM\n3H0vh9y1D94T30fh1g+DJ4fnfZ7u2oTSJb+A4toXo5zpByt1ToW6kVVWsdiOP97dCBBDCwHBDK0i\nMSOtjMXmu5oitvo4JQNmbSdkgBUzyyJhhsmODrnQQ21yGhFB9PQjc80b4F59Mwqf+yuU7v0ykJ+q\nfqyTg97yYyhf8Tvw3FXQngeo+ixiEq5dEVho5FtoIhAzgoAfMyMFzQJaaQhN/rx/DufOCikSgtZ6\n0cv60SZnoaWZcAngSQd6RIIL7fnVICGRe/sfwN13A4rf+xzU0/cBGANkFrT1euCinwRveDFQ9oBy\nfTK00eh2+JaVnwCQyURAIEdMGkKQscxIQGsTNwvGZQOAEGbhHhFaY+35eQFWzJYlxZvejewd/wYx\ncry5OyYBYCPUkbPgQpPjYgSIVTng7FfAqwZBmdWN36WUcC6+BnLnXnjPPgDvidvhXP1qYMOLoJwe\nwKt/mUm0IhglkwL+SmLxQaLMIlyJTAiOiZkO5/6b+ySIRLW72WZQsyaVptB2TTTBFVEzQysFz/NQ\nKpVR9srwyh7KngelVGLl6drehqAa3KzvKaVEpjgF9zsfh3jsu0BhekHH6e7JL+yFCRe0ahsy1/83\nqEMnkf/kBxb2/BjO7gKoY2EfPWUl5PpuiC7XmC25jRDn/xao/8Ug2dG0gfbMHFo+nqeiz7VcDj9b\npZR5TPSsebcbyFMQH5NSwnUdOI4L13HgOI4fNwsC+v7il+GKS2YJOa01dHBu6WhJxXgMzqxAFiUa\nWpRFHZi1zJYp3LUK/ObfBV3xauAHt4H3P7BgUasFWncpnAveBOeit4Byq6AO3Vr3fcy676yE6MtB\nrM6BZKxiv3AS+rE/AK15KWjTG0EDvqi1CRT/TUFXQLxsI3hA1P7EAISUZtVyrcECfmDTL8CNjYdv\ncSFbNFbMlisEQDig86+Cs2MPcOBHKN/2YTPHqx50DMB90a9C7n4tqGejsQaahQDE6k7I1TnAnX18\nEQ/dAx57HLTqUtCOd0H0Xdq8Y2wkFP8z6E2qfA+C+Fps1TH4i5gIiaiLM2hxCuJlfrytDdcCtmK2\nXIl7Lx09cC67EZkrXony7R9B6e7PgyfmLyuohgC3E3LwxXCv/xOIVdvrdLC17546XTgbe0AZWdtz\nvAnw0A/Ao4+Ct7wdYsvbgMxqENX4/GVB3FajsAk9fh/5NWaB8KeFj5bjSmQLwYpZm5F9/a/DuexG\nlO78rFnsY2q0tifKDOT2GyAvfhucHa9o7EGmQJ0u5OocqDuTdClrRU2DD/0L1Jk7IAbfBKx/BSi7\ntr3Mj5R+y8TdRInA/2yPaVesmC1XZjsnhYDcdik6fvr9UNe+EYXbPwb1zH1zbkqsvRTuS34LYsPl\nQG5V/Y91zp0T5LouiJ4MyK2DNTVzCPr5jwAnvwax9R0Qm1577ttcRkT9m8EtkbPZxjoGwIrZ8mWe\nJBllOuCcfxW63rsHpbtvQ+nbn4AeOQVov7CThImL7X0nnMt+ETTnVIcGIAiiy4Xc2A1y6uwSchmY\n2g/91J+BT30N4rz3Al07QKKJa202gIVUiSWXWlwZWDFrcyiTQ/YVPwPn0peidPdtKN/7ZVBXGfK8\nm+Hs/VmIVduafEAE6o65lA02F3jkAagf/SZo/SshNr4W1Hsh0FbxNEuAFbMVgly/Hbk3/gYy170F\nlGVQzyDIyTb9OERXF5zBRcbFFkt5DHzs8/CGDsG58i9AucYX3FqajxWz5coiDBpys5AbdtT/WBZA\n6REPahjIXJIB9ZgaqEbCmsH5MtTZaVDvKUjlrSDHa2VhxWy5soz7J9RRD4VhBWerA7nNgVzVGLdP\nz5ShxwrQk0VAMai3IbuxtAhWzCxLAs8wys+U4R3x4Ox24Z7vgmR9bCZWGmpoBnq8CHjLed1My0Kw\nYrZcaRNfiWfYrMJ+2IO7LwO5RoIcLLw+zO9H5IIHdXIKXKrPOJ65aJOPoG2wYrZcWcZuZhp6XKP4\n/QLkZgfudgdinVyQpaany9CjeejJxY+9ro2gacjSalgxs7QOGlBHPKiTHuQGB5k9GYieubOeXFZQ\np6ehp0uAaoLIBFoWdoEn5/aHgxCZreQ1GStmltaj7CcJhhTcC1zILQ6oIzbqmRmsGHqyCHV2pqlx\nMaNl0RBFCnslIytSEEEzJ55haTxWzJYrKyBgw3lG6ZESxGEPznYHznYXcACeLEKNFcAz5aXRiRRr\njASBtJkZprQywhtaZ1bQmoEVs+XKCvpu6FGN0ngJ3gEPclcZ0GVAL90b4M+CNRaZCFYfN0IWTq6I\nxsFaQWsSVswsywMN6AkNMVle8KTauuFbW8GfwUK9UgowS7CMZu9r6NjkYe2vRB43p+NuqKUeWDFb\nrtjvQNMxMf9ofpgQBMECLCSEZEhIgM1qe0QIVxoXZi2lUAgjS81ST6yYLVesx7KkBLEyKQQggUii\nCKQICgoEHYpatH4AQEjJdFptO2esmFksCyKaCyaEKRsxBpcDgKBJh0KntAZpDSKzSC/I7xVlMoKW\nmAZLs/xtqRUrZhbLAgjCZkGZSCBowUIjWkcZTqE1lFZQyr9NG7EzggZENhqnyJcVtIVixWy5Ys/1\nJSOYtx/8LYQAiJMrKPmL8wolQORBa4LWwiw1SBxbrDcSx2Al82hHS/QClylWzJYrNma2pFQOlZRE\n0AhESRjXkjQ0qdDl1EpBBO4nUSRoJtWZLOmo3F/jX9Kyx4rZcibWUmNP9+YTLCBiXE+O1ZkBQpC/\nwrgACQWhNLQwlpnQJoam/N8cmmeRhSaC7gJLzVgxa3lii4qFK4VRlWWwXKBeBZ4SgG7+8cvtN4Ky\n9R1qRrNcSIx1xmFngBYaWunQ/dRKg7Ty3U8Oa9KECNYJDfo+22uBqUZixWwZEF/ENS5iFJzwwYPi\nT2hRRI8CbSpDnXHAExLwGnywmW7IwavhvPi9kOv3Nmw30edCIOIoDsZsyjWITJGtNmKmSBuRUxpa\nmKSA9rsHAkGjBS1hYrFi1rJQ7GeULQtPdv+EF8vQSiOXITeWwX0KekQaUeP6vwYxeDWcPT8Nue3l\noGxP3befRmW2EzAuJ7F5iSY5YMo1hG+xKR2ImWmYDzoLhIimcFhRmx8rZi1J0swSJoDin+CRiJnb\nw05BVApgK0MCoG4N6tTgKQV1wgXKdVrkJNuLzDW/B3nRWwCno+liT0QmPxOuMM4gCuJpAoIJmsh3\nMbUfQzMZTn8DRshCCw3L40NdYqyYNZtZ+42p6q/4VIagBzD4LaU0fwcnPC3PRV5JANSrIbqLUGcc\n6HEJlBZpiXQMQG65Fu41vwvRt7Xux7oQ4q1PVfcQQ8ayniQ0hC9mjLhb6osaCatlNWDFbMmh6n/5\n8ZLAzRAkIEIhk5BChlMazBci/uVfpqe9AMR6D6JPQY9K6BFnQa6n3HUTnEt/CmLLS0GitdbFDCy1\nMPnsN50L6cfIWECTDrOhwXMSFynras6LFbMWIWGNBSIGX8TIxE+kY4RMOhJSOhBSmhE0IrDKlqd1\nFkAEoIMhch7Eag11wgXPzO16Ut9WuNf+AeTOVwLCadn4YTDUkeDXcgT/EkGMTYLB4MDVRCyM4Lua\nLfrSWgYrZk0kiG4xOLxCh/dVDPsL4mPB/zKwyqSM3EyKYmit+iVeDEbUNOT2IvSw73oWkpYJ9W6G\n3PZyOFe+G6Jn05Id60JIK+MIuglM8WxlHUaU9Ak+33b6nOuNFbNmQnFX0Be0oNyCkBCnKEYmE3Ey\nKQSEHy8TQlSk79vrRCcJyHW+6zkmoYcdwO2Ec8nrIC/5KYh1l4DE8jyFg4JbkxyIEjecCKpWWtrt\n9fnWm+V5JiwzKoPBxsuIRIyIIClwGWOBfiEhpIQUBCFkeHu8Fina9PJ2MeeCsgyxzoMYUMi+/mOQ\nu24ESCx7KyX43MLptABiq6X4j4nXrzX3+JYbVswaTcy1IN/60qRBHLfGfLEK3EoZ/R0IWFBbJshP\n2QMryvUgAuAwqG9jywX4z5VEIXRQ1hH8u/mHs2yxYtYkTOzD79NjYUoSIFKtMRlzI4PiSTOZwaTr\nzfZWjpCtNOwnujismDWAZMNRENT3e/V8SwxAQqyCLKUgEXMnTTaTYoFgIPht3Q6LJY4Vs4YRxcPi\nGcqgojsK8vtWmPRjZDFLzAhgpXhZEbNY0rBi1iiCTGXgRvqjYRAbxBeJGSXiZIE7WelKWpfSYpkd\nK2Z1J0qwB+NfSBAEJATHRyxTWHpBwiyMEW8eDwolw98Wi2VOrJjVmcgVjKwvMCCI/b67QMySAf7K\noll/a1bILJYasWLWQAQR2C9sDVfi8WNggkTUthSLiVmX0mJZHFbM6kjQmhK3rqQQ0LHKoVCwUuNi\nNrhvsSwWK2YNIu4qCslhYXfkZtrgvsVST6yY1ZloGbLIyqLYj7hmhQJmhcxiOWesmDWAZHsKUF3T\nbd1Ji6XeWDFrEGETMRAfgmCzkxZLg7Bi1mBSDTOLxVJ36rSChMVisSwtVswsFktbYMXMYrG0BVbM\nLAtCrN8GsXYrIJp86ggXNHABRPeG5u7XsmygsM2m+SzZji3nhh49jdL3b0P5gf+EPnUQWMA5JAdL\nEP1qQfsT6/dBXvhmOJe8HeTkFnq4luXHolJmVswsi4K1hj75Akp3/TtK3/10zc9biJhR1zrIPe+E\nc+GbQD2bQGQdiRWCFTPL0qBOPI/8v7wf6uizgCrP+diaxMzJQazfi8wNfwHRv7uOR2pZJlgxsywd\nemoU5fv/E6V7vgB9bD+g0wVrTjETLsSmq+Ds+WnIHa8AOdkGHrGlhbFiZll61NmjKP/wqyh981/B\n+cmq+2cVs45+uC/9IzjbfwzoGLCN9ysbK2aW1kEdew6FL/0NvKfvA0qF8PYqMcv0wNl9E9yXvR+U\n7V2CI7W0IFbMLK0FeyWUH/wGSvd+BeqpHwCIiVmmG3LLS+Fc/HaIbS9btiuTWxqCFTNL68GswZOj\n8J64C4Uv/h1E1zHIXdvgvuS3Ibe+zFpjljSsmFlaGz16BurYD+FccCMo07XUh2NpXayYWSyWtmBR\nYmarEC0WS1tgxcxisbQFVswsFktbYMXMYrG0BVbMLBZLW2DFzGKxtAVWzCwWS1tgxcxisbQFVsws\nFktbYMXMYmkSZ4eGMTk5tdSH0ba03KgCZg2emQTKxabul3r6QbLl3g5LHSmXyyh7XsO270gHmYyb\nuM3zPAyPjOCpZ57F0PAIXnbtS9DT092wY1jJtN63t1RE6dufhPf0vc3bJwl0vusvQWsGm7dPS9M5\nevwEnj/wQsO2v2XzIC664HwAgFIKZ86exaEjR3Hy1GkUCgW4rjvPFiznQuuJGWvo04egDjzSvH2S\nAMcGCFrak5mZPIaGhxu2/dWrVwEAPKXwyKOP4eDhwyiXG2cJWpLYmJnFUmdYa0xOTVkhazJWzCwW\nS1tgxcxisbQFrRczIwJ1rwL1b6zt8eUieHKk+nY3C+rpr3GfArCZzBWL67p1Cc5nM5k6HI1lsbTe\nN9jNIvPKn4N7zRtqerg69AQKt/z3qtvl1ouR+8nfr3GnBLF6/QIO0tJObN2yGbt37Djn7eRydp3P\npaTlxIyEhFy3FcDW2p5QmEnfTmcPnB1763dglralsyOHgYEarXhLy9JyYtZKMLNxY70SACO0yHSA\nRGNDjVzKA14ZDICcDCiTq9+2CzOA9sIFGERn/VdHYq2B4gyYNQCA3BzIbawLxlr5+2QQCSDbYT6v\nNoGZoZSC1uY9FUJASmkXS45hxawCZgaPnIS3/0GoI0+BJ4bBxby5Uzqgzl7IwfMgd18OueXCmroG\n9OhpqCNPV90u+jdAbrnQPGZqDGr/A/D2P2j26de9USYHMbAJYvMFcPdeD+roWfBr8l54FOrwU9Cn\nDkGPnwG8cnQMqzeAVq2F3HIB5M7LILpXLXj7AMDlItTRZ6Ceewh66Dj0+BCgzWK/1NEN0bcWcude\nyPOvqnkfenIU6vCTgEqWOIg1gxAbd4GEgJ4YhvfYneb1jZ0FWANCgDr7ILddDOfCqyE37lzUa1pq\ntNaYmJjE0MgIxsbGUSgWoJR5Tx3pIJfLon/1aqxbtxadHR2zCpvWGiOjYygWk101jiPRv3p1TfHC\ns0NDKJVi540UGFi9Gpka4oQjI6PIF6rrONeuGajp+bVixSyGnhxB8Ut/j9L9XwXKJUCVUx9XFhJw\nMpDbLkbHz35g3i+L99yPkP/X91fdnnnJ69Hxc3+G4l23onj7R8FTY4BvBSYgAqSDQs8Acm96L9yX\nvL4mq0Md24/CrR+C9/zD5rWoWeqeSACOC+rshXvFq5B70/tAnbW33KiTB1C45X/Ae+ERI5RaVT+I\nCJAuqHsVMte9FdnX/grInTvGpI49i/y/vB+cn0zcnvmxn0Turb+N8uP3oPD5D0OfPZb62sr3fQXU\n0YOOn/8g3MturPn1tAL5fB4/euQxHD95AlpraJ2+mJkQAkIIbN2yGZftuRS5XLUVr7XGs889h2PH\njocWOQHIZrO45uoXY/26tfMez3e/d7exemGWVZNC4PrrXlrTc+9/6CGMjU+Ezw32/+Y3vn7e5y4E\nK2YA2Cuj/ODXUfjCX4NHT8//BK2AUh7quYcw9cG3Ivf69yDz8neAOmexmlin9prqqVHkP/M/UPre\n52YXGgBgNm7n6CnkP/kBeAcfR+6NvwHRvTr94eUiyvd/DYXP/VWVEMx1fDx+FqU7bkHp3i8j+4p3\nwn3JGyA3bJ/1mPT4WRS/8ymUvv7PNeyDAa8EHjuD4lc/Bu+pHyD31t+B3HU5SM4izKzB5WLVe8fT\n4yh85aMoffvf5u7hVR7gZECLtDaXgnK5jAMHD+GxJ56EV0MfqRE6jRcOHsKZM2ex99JLsGXzIGTs\nPXUcBz09PVC+ixowk89jcmoK69aumdNdPXX6dGgRxvc7NDw8r5jN5PMYGR2run2gvx+ZOrd3rfg6\nMy6XUPzyPyB/y3+vTcgq8UoofPX/IH/rhxbcEqX2P4jS3Z+fW8iq9ldG+c7PovDFvwUXpqvuZq1Q\nuueLyN/yF7UJWRqFaRS/9o/I/8sfQU9Vn4gAoKfHkf/0n6P0zX9d+PaZoV54FDP//IcoffdT4AU2\nf3tP3oPSNz5e0zACsXodxNoak0lLjKcUHn70cTz62OM1CVklU9PTePDhh/HoE09Wic/agQGIFMGq\npb3r7FD6Y46fODHvc0+fOZN6++ZNm+Z97kJZ8WJW/MY/o/iNfwYW+8UHgFIB5Xu+gOLtH13Q03h6\nfNHTQcr33o7yo3dW3a5PHULpO58CzrXXlBm0eiOoqzpBwMUZzPzNf4H3yB3pLmWtuxg5icJXPoLy\ng19f2PMmhmrer1i/A9Q7sJjDaypKKdx3/wN4/oUXqiyohVAqlfHMs/vx8KOPJ25fs2YgNXE1PJxS\noxnD8zycHRpKvW9oeASlcnooJuD06bNVtwkhsHFj/UuhVrSb6e1/EKW7Pm9coEqEgOjfBFq9AZTt\nAGC+xDx6GnroWPXjmVG887OQF1wF55LrFp5lEhJiYBOoaxWoqw9cnAaPD0GPnk6Po5ULKN/7ZbiX\n3RgeHwCoFx6FPnWw+vEkQAMbIddu9d1hMq9nehw8OQI9cjIhENS3Ftkb3mEyg/GXWSqg8O8fgjr8\nxOyvpaMboncA1NkLLubB42eNcKdRmEbxqx8zCYjB8+Z6h2bHzYK6+kCZHLhcBOenAN9qdS64quUz\nfoGbePzEyVkfI6VERy6HTCYDzRqFQgGFwuwXwsNHj2D9urXYPLgJRATXcbB5cBMOHzmaeNz4xASK\nxdKsNXLTMzPI52e/MB47dhw7d2xPva9ULmN0rNqy7+3tQTZb/5q8FS1mpe/fBh49VXU7dfUhc8M7\n4O67AWL9NlBHjynTyE9BnzmC8sPfRunOz4JnJpJPLEyjdOfnILfvWVCchjp7kXn1L8Ldcx2odw2o\ndw14ZgI8fALeM/ej+O1PgMeqzXXv2fvhHXwM7oVXR7ftfyB1H/K8FyH3lt+G3HpRokxCjw+BR09D\nHX0apbtvgzr0BMAa7uWvgNx+aWIbzAzvqXtR/tE301+Ik4F79c1wr3glRP9GUE8/OD8NPXQM6uBj\nKN11a+rr0KcOonj7R9H57r+u5e2KEA7k+S9C5rq3QfSvB+W6jHhOjsB74TGUf/AlOBdfO+9mCsUS\nxicm5n0cYOJPXZ2dCzvO+fZfKOLZ5w9UuYYAIIiwft067Nq1A91dXcjlctBKY3pmBmNjY9j//AFM\nTlUPfCwWS3jqmWexceMGOH78bPu2rVViBgDHT57Erh3bU49tamoaMzPptZwAcOjI0VnFbHpqGqVy\n9YV4VV9fQ7olVqyY6eHjKN93e/UdJJB70/vgXv8TCbOciIDOHsjtl0BsvRBizWbkP/s/gVI+8XTv\nqR+Ah48DtYqZk0Hnb34McueehBVE3auA7lUQWy8E9Q0g/8kPVLuOXhneg99MiJk6+mzqbrKv/RXI\nnXurrBTRtwboWwOx7WK4L74ZpQe+hsKnPmgSGjGLz+yvZC4AaXE0x0XHz/wJ3GvfmMy09q2F3LAd\nziXXwrnoGsx87LfB49WuR/mhb6H86J1w97089fjTcK+6CR0/+6em9i/2upgZzr4bkHvDrwPO/EHm\n558/gAMv1DbnbNOGDbj+upfWfIy1cPDIYUykiCkBOP+83dh76SVwnORXtbu7C+vWrsGWzYO49/4H\ncPpM9Xs6PDKCg4cP47ydO8Njd6SEVyGax0+cSBUzZsbY+NicAy2HR4YxMzODzhSBn5qeQqmUFDMi\nQl9vbyJBUS9WbMyscPvHUt1L57Ib4b70zXMWxpKQxgK58tXVd5aLKH3/C7UdhBDI3Pgzxlqi9P0R\nCWRe8nq4l78y9f7y49+ruCU9hU9ubk53i4hA2Q5kr3srev/2PsjB3VWP0SOn4D1zf/rreMU74V59\n86wlI0QCzu7L0fkrH5ol68tGKGcph6na5eB5yP30H4OynVWvi4hAQoDcTE0uJgPQmmv7Py0kcQ6U\ny2U8/cz+1Ps2btyASy++qErIAogInZ2deNFll6GnO72U5qmnngmTCUSEtWvXVD3m1KnTqQkHBlJF\nMo5SOjVbycwYn5isGoPkOA4G+vsb4vqvSDHj/FR60DnXjY53/HFN1eqUySH7mnel3ld+aBY3rHIb\nnX2mELaG/WVf9+7U23nkJPRwlFWarXardPetNWc3qywyn/JdtwLF6gyqWLMF2Vf+3Lx1Y4CJYWVf\n957U+9SRp6HPVLtBaWRf/x6Izr6aHtvKHD9xssp6AUwd12V799RUVLpqVR+uuCy9dW96ZganTkeu\n/Yb11YF3T6lUQfI8rypBUFlgG5RoVKKUSo2XuY6D/tXpJUXnyooUM3XkaaBYHQdwzr+y5oZzLpeg\nh46BVm+ovm9yBN4Lj867DepeBbljX037E+u3Q6xLLzGIx+7Eum2pjynfdztm/vVPoKdmCcTXQOnB\nb6Te7r7srRCr1tW8HfeqmyA2VDd28/SYqfifB1q1Ds75V9W8v1bm2PH08oatW7egr7f2VrNNGzdi\nY4pQAcDJU1HJ0epVq1JdvDRBOn36TJWLuX5tdV3Z6NgYChXdBUopjI1Vn2um6r8x48NXpJjpEwdS\nb3cumT1YrKfH4T3/CEr3fBEz//f3MflfX46Zv/+11AQCAKijz8x7HKJ/AyhTW1aHiCAGz59lX1Gc\nzNnzMmAWV8/70bcw9Sc3o3D7R+E9/3DUplUDenwYPJLyxXMyyFz3lpq3AxiLVO6+ovqOYh761EHT\n2zkHYv0OkLP8x+2USmWMjldbLwRg5/ZtC3LFiAibB9NrtyYmJ8LkQldnJ3pTFlQZGRkN+z4DDh05\nkvi34zjYtm1L1XFNTU2jWJFZzRcKqYmJrVu3zP9iFsmKTACok89X3+hkqiwfLkzDe+4heE/eA3Xi\neeiTL4DHh0zF/DzoY+lxkDi0AGsGwOwdBjGcXZdDbr3IZCVT4KlRFG//KEp33Qq55UI4e66He+Vr\nIISDZHAAAA2hSURBVOaZ/aYOPZ56u9i4c9ZOhFlxs5CDu1GWTlXBsJ4cMaUoczTXU1fvrIK9GLLZ\nDHI1lgrUM5OZL+RTM5jZXA7dXV0L3l7/6tXIZNxEDyVgSiSKxSI6OzvR0ZFDZ2cXRiuspqmZaeTz\nBXR1mddXLpdx4mTyQt3V1YnVfavQ19uDsfHIG5iemcHk1BT6+iJLMq3MxHVdbFi3sHN+IaxIMUsb\n5kgd3Sa1rzXUocdQvudLKN116+J3cg7FpOeCWDOI7Kt+ATMf/8PZOwuYwWNn4I2dgff4XSh8/sPI\nvuaXkH3NL1ZlBsOnzKTH2+TmCxZ8jERk3FI3Wy1mZ46AS4U5J4WIvrVAHbNh5+/ehT2XXFK37dVK\nsViEUtUXxr7enlmD/nPhuA6y2VyVmJXLZRSKJXR2dsJxHKwZWF1VvV8sFFEoRmI2PDpaJbTdXV3o\n6Mhh08aNCTHTWuPs8FDCMkzrDli7Zk1DV6hakWKWBhfzKH7tn6COPwcemb14MQ3q7AWtGYRcvx3y\nghfDueRayDWbG3Sk8+O++MfRwRrFr/0j9MkD6UXBcUoFY63d+VlkbnynqTGryGZyfpY6rBqC/mlQ\nzwDIzaa2ZFkWh5QSbg0iuGZgAERkaid98oUCpqanMdBvLPRTp6pb+/p6e+G6LrZv24qnn92feP6J\nEydx+V6ThCiXyxhNSShs3NDYAahWzAJKeXiP31X7490s5I49cM67Es6FLwb1bzQV/C0yfjtz9c2Q\nmy9A6bufRunuW+cXNBiLtXj7P6B8/1eRe/174Fzx6rAJnEfSY4OirzrVb2ltBvr74TpOohVJa43R\n0TFs27IFnudVBe+JCP39JpzQ3dWFrq5OTE1FF6KJySlMT0+jq6sLQ8MjVS1ZruNgzUBj28pa45u3\nHCACSECs22pqqvbd0PKjtuXgbnT87J8ie/OvIP+JD8B75r753V+toU8eQP5TH0RHpiMsYp2tv1EP\nHa/zUVsajeM42LJ5Mw4cTLa9DfllGFPT05iaTlrMgijMZAoh0L+6PyFmzIzjJ0/h/N27UpvL+/r6\n0NFRvyGjaVgxm4tsJ8T6bZAbd0Hu3AvnomtMe9Mym2Aq+jeh833/G+rgEyg/9A2o535kkiBzZDN5\nZgL5f/3/IH7345Cbzwf1pIsZL7JRnqdGwSmtLiRdc+FYwSilEi5crWitUxMKBKp6S7dsGawSs5HR\nUSitMTMzg5l88tzo7+8P+ymJCOvWDODosWOJ4zx+4gR279yB4ZHqmHRfb09dBzGmsSLFbL6Vn+Su\ny+DsuR7OrsuM+7h6Q8PHPjcaEhLOrn1wdu2DHjoOdfIASvfdDu+xu4BCdQodMIKT/9SfofsPPw3q\nSs+k8vDiLDM9dDy11o96+9ui7KIWXDcDmdJpMjOTX9TkjGKxhOmZ6hik4zjIVJy/69asQS6bTdSH\nBUWyY+MTVR0BWzYPhn8TEXp6jDjFp9eOjo6ZmrOKMg0hBFavWhX2iDaKFSlmoi99oJxz1Y+j422/\nA9Ff/1lLrYRYMwixZhDunuuhzhxG/t/+FGr/g6lxNXX4KXiHn4QYGEzZEqAOPg49PQGRMipoNrhc\nhD57JLXERfRvWnRSYbnRkcumFrDO5PPIFwoLLgMZGx+D51VbZq7rIJtNipnjx7COVWQdT50+nchU\nBlQ2k3d3dSKXyybEzPM8HD95qqqjwXGcpiwYsyKLZsXWi9LvyE+1vZBVItdtQ9f7/hHZ1/wSkNbG\npBX0qYOQg+fNug5p+d4vLWifPD6U3uOZyUGs3dLwBWNahVwuh97edIv34MFDC9qW0hoHDx1Ova+n\nuzu11COtT/PI0WMYrugGWL1qVdVU2K6urqp+UKU1jh47hmKFmGUzmQV1MyyWlXHWVODs3Adkq696\n3rP3Qx17bgmOaGkhN4PsG98Ld1/KnHzmcMqF+5LXpT6/9N1boEfTJ4qmPv6B/4Q+U/3Fo85eyM3p\nXQ7tymxV+4eOHMXEZO0DQ48eO47hkdHU+zbMUhLRv3pVVd3XxORk1eIja9dUi54Qoio7ycwYH5+o\n6iTYuGH9ourmFsqKFDPK5JC5+ubqO8pFzHz0fWCvtskNAMDKC5dUawWYGXrkpJnrtZAgsnQgZquN\n80/OzHVvAVKsJj18AqV7bpu3DYmZoY4+g+IX/jrVpRXrtkJuu7j2Y24Dtm6pbg8CTK3WI489Puf4\nHcC8p1NTU3j0scdSP+9sJoNNG9NjxJ2dnejunr/TYO2a9OTPpg3VfclpbB5MD1HUmxUpZgCQvfnd\nqe1BeugICv/+V7POvg9graCOPYfCF/8G3lP3toyg8egpzPzj72Hm796D8v1fM8MXaxI1hjqR0uZF\nBPjL24m12+DsvaH6MVqhdMdnUb7nC7NeCFircO7/bGSuf3sNx9leuI6Diy9M76I4eeo0nn7m2TlH\nU09NTeGRx5/A9Ex6ZvriCy+YtZC2I5dDd+fcYtbV2YmelF5OwEzrmG8V92wmgw3rG9fCFGdFJgAA\n0+TtXvVaszJSHGaUvv8F6NHTyL3xvalujx45idJdn0f5oW9AnzoI78kfoPNdf9kSLlLpgf+EOvQk\noMrwDj8JufkCuPteDmffDZCbdqU+h2cmUPj8/4L3xN3VdwoRvS7HhXvVTfCevb9qzQSeGELh1g+j\n/OQ9yL7q5yG3XQLyByPqqTGUvvMplO79Cng4fUqEc/G1cK941eJf+DJm5/ZtOHzkaFVtl1IKTz+7\nH2eHhnH+ebuwcX3krpXLZRw9dhxPP7t/Vne0r7cXO3dUTycJCALzR4/PnpHu6upER8rydQHbt27F\nM/tnD83MZhU2ghUrZgCQeflPwdv/APTJiimj5SK8R76LqUfugNy5zyxeu/sKqBcehTr0uMn8xdDH\nnsXMR96Lrt//JMTq5lyF0tBjQyh+5SPRep/FGagDD0MdeBj40t9BbNgO59KXgTp6QD2roc8eBZ89\nhvJTPwhn5ldCmY5woWIignvFq1D+0bfgpcxs4/wkvIe+Ce+hb4J6BiDWbQEXpqFPHZp1DVLAtDbl\n3vZ7y778ZbF0d3dj184deOyJJ6usaKUUTp85g9NnzsB1XfT2mhHuExOTc67gJITAhRecN++4ndlc\nyIC+3l50dKTPtwNMycZcYrZhQ/O+DytazMSmXci+6heQv+XPE6t8RzDUC49AvfAI8O1/m3NbeugY\nZj72PnS+528XNNurXnC5iPwn/3T2VZm0gj5xAKVZxh+lIiRyb/3tRNM3OS5yb/kd5EdOQh1Mn6QB\nADw5DDU5/zJmyHQg+7r/AjGL1bgSICLs3LEdY+PjqTP6A8rl8ryrKQXs2rEdWwY3zztGKCiGrVzt\nPCDo1ZyNVX19yOVyKKSsWJ7L5bC6r3lrlq7YmBngj79+yevQ8c7/BtShUFMMDNZ1NM3Cdi4hz7+y\nrjVa7tWvg3vNG6tul+u2oONdfwm5s7bBknORfcOvIXPtm1ump3Wp6MjlcOXll826OMhCuOC83di7\n59KahiBKITC4Kd0VJKJ568OEELNad329vfPG1OrJihYzwIyZdq99Mzre9T/9qbGLaKXJdiLz2l9F\n569+GGKJ1mgk6SD3ml9E9x99BvK8F6WWntSM48K54lXIveW3Zh2FLddvN4J20TXAgoWIQD396PjV\n/4Xca34JlKvvakfLlWw2iyv27cWuHdsXteCH67q45KILcfm+vQta/WjXLHG1np6eeevDTJ9m+jy7\n3ia0MMVZ2ZdDHyJC5sqbINZuRfnuW+E9eY+Zqz9XFpAExMBGyPOuhHv1zXBiKyQtJXLLBeh638fg\nPf8wvEfvgDr4BNSxZ9PX3qwk1w1n516zqMuLXwvRNfeMfbluK7re8zco3f15lB+5A+rgY3PvRwiI\ngUE4l16HzEvfPHvx8gomk8ngRZdfhrVr1+DwkWMYGh5GeZ6FdjtyOaxftw47tm/D+nVrIRZYdLx2\nzUBVaxMAbN08f0kFEaHXHw0UP04hBPpXrVrwsZwLtJiG1jpRlx3zzGRqSQF19UJuXHgchstF07t4\n+El4T99nFtT1SjCHS6CuPojN55vEwOD5EAMbQZnZA6QAoCdHoU8fqj7G3n7IWWb2p6HOHAFPVMeh\nxLptEL3V7gB7ZfDEEPT4kEleHH4K+vj+8LUAANwsxMAg5O7L4ezcB+rfMO/U2ar9sAaPD0OfPgTv\nuYegDj4GHjsd7SOTgxg8H+7F10Bs3A2xdrCm/kuemYQ6fRCoaJ4WfWtBazbNuqLVbMzMzGA6ZQ3I\nzs7Ouk6Q1cyYnJisWjOSSKC3p7tma6VYLGJyagpDQyMYHhnBxFSUtRRCoLenB+vXrkV//2p0d3Wd\nU2Hq6NhYVUKhp7sbuTkymQGlchmTk5OJYlkiQm/Poi2zRU0aWPZiZrFY2o5FidmKj5lZLJb2wIqZ\nxWJpC6yYWSyWtsCKmcViaQusmFkslrbAipnFYmkLrJhZLJa2wIqZxWJpC5aynWllrydmsVjqirXM\nLBZLW2DFzGKxtAVWzCwWS1tgxcxisbQFVswsFktbYMXMYrG0BVbMLBZLW2DFzGKxtAVWzCwWS1tg\nxcxisbQFVswsFktbYMXMYrG0BVbMLBZLW2DFzGKxtAVWzCwWS1tgxcxisbQFVswsFktbYMXMYrG0\nBVbMLBbL/9uwAKOF2SgYBaNgWAAA07/vhmwiy/MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1124a19d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_img_from_url(url):\n",
    "    from PIL import Image\n",
    "    import cStringIO\n",
    "    import urllib2\n",
    "    im = Image.open(cStringIO.StringIO(urllib2.urlopen(url).read()))\n",
    "    plt.imshow(im, origin='upper')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plot_img_from_url('https://www.tensorflow.org/_static/images/tensorflow/logo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data corresponding to a linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 1024\n",
    "x = np.random.normal(0.0, 0.5, n_samples)\n",
    "y = 2.5 * x + 2.25 + np.random.normal(0.0, 0.05, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "As in tutorial 1, we are about to solve the estimation of a * x + b minimizing the squared loss of error with respect to y.\n",
    "\n",
    "This time however, we are going to build the flow computations linking the data (observatoins of x and y) to the loss and let Tensorflow compute the corresponding loss gradient automatically.\n",
    "\n",
    "## In this tutorial, an in this tutorial only, we will mimic the usual matematical notation of variables in upper case and their realization in lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders:\n",
    "\n",
    "The first thing to build is the pair of buckets that will hold the data for x and the data for y. At a given step of our stochastic gradient descent, we will only take batch_size elements into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a placeholder for the regressor:\n",
    "X = tf.placeholder(shape=(None, 1), dtype=tf.float32)\n",
    "# This is ready to welcome an numpy array of the same shape\n",
    "# when we choose to assign value to the placholder.\n",
    "\n",
    "# Let's do the same thing for the predicted variable:\n",
    "Y = tf.placeholder(shape=(None, 1), dtype=tf.float32)\n",
    "\n",
    "# A shape of None is adaptive and this dimension corresponds to the size of the batch\n",
    "# We may want to change the size of the batch of data without having to re-instantiate\n",
    "# the placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables:\n",
    "\n",
    "In the linear regression model we consider there are only two variables: the slope A and the offset B. Let's instantiate them and apply the model to the regressor X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('linear_prediction'):\n",
    "    shape = [] # Shape [] means that we're using a scalar variable\n",
    "    A = tf.Variable(tf.zeros(shape)) \n",
    "    B = tf.Variable(tf.zeros(shape))\n",
    "\n",
    "def build_linear_predictor(X):\n",
    "    \"\"\"\n",
    "    Build an operation that predicts lineary with X as a regressor.\n",
    "    :param X (tensor): Values the prediction is based on.\n",
    "    :return (tensor): Degree 1 polynom in X with offset B and slope A.\n",
    "    \"\"\"\n",
    "    return A * X + B\n",
    "\n",
    "Y_PREDICTED = build_linear_predictor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss:\n",
    "\n",
    "Now let's create a loss function encouraging the predicted value for Y to be close to the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOSS = tf.nn.l2_loss(Y_PREDICTED - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing a gradient:\n",
    "\n",
    "Tensorflow is a symbol to symbol automated differentiation engine. Before we can evaluate the gradient for LOSS, we need to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GRADIENT = tf.gradients(LOSS, [A, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a session:\n",
    "\n",
    "Now that we have created the symbols of the computation flow, including the gradient which is our quantity of interest here, we need to create a session that will enable the computation of numerical quantities such as the value of LOSS for a particular choice of input for X and Y.\n",
    "\n",
    "A session also keeps in memory the state of variables which we will update as we improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'global_variable_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-46605fffaa51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variable_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'global_variable_initializer'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variable_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a quantity:\n",
    "\n",
    "We can now ask the session to evaluate any tensor we created. What we need to do however is to create a feed_dict which matches each placeholder with the data we use for our computation.\n",
    "\n",
    "Let's just start by computing the loss corresponding to the current values of A and B with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot_loss = sess.run(LOSS, feed_dict={X: np.atleast_2d(x).T,\n",
    "                                     Y: np.atleast_2d(y).T})\n",
    "\n",
    "print(tot_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the corresponding value for the gradient of the loss we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_grad = sess.run(GRADIENT, feed_dict={X: np.atleast_2d(x).T,\n",
    "                                          Y: np.atleast_2d(y).T})\n",
    "\n",
    "print(loss_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can implement gradient descent if we update the values of the variables A and B using the assign operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "print('Value of A: %.2f, value of B: %.2f:' % (sess.run(A), sess.run(B)))\n",
    "\n",
    "sess.run(A.assign(A - learning_rate * loss_grad[0]))\n",
    "sess.run(B.assign(B - learning_rate * loss_grad[1]))\n",
    "\n",
    "print('Updated value A: %.2f, updated value of B: %.2f:' % (sess.run(A), sess.run(B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our learning rate is good, we should have a lower loss now if we compute it for updated values of A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot_loss = sess.run(LOSS, feed_dict={X: np.atleast_2d(x).T,\n",
    "                                     Y: np.atleast_2d(y).T})\n",
    "\n",
    "print(tot_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent, without having to give the gradient as input!\n",
    "\n",
    "Let's now code again a gradient descent algorithm but this time using Tensorflow's automated differentiation.\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(tf_session, learning_rate, \n",
    "                     LOSS_FCT, VARIABLE_LIST,\n",
    "                     feed_dict,\n",
    "                     precision=1e-4, MAX_STEPS=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent using tensorflow to compute the gradient of LOSS_FCT w.r.t the variables in VARIABLE_LIST.\n",
    "    Here we are using all the data set. Feed_dict remains constant.\n",
    "    :param tf_session (Tensorflow session): session that will be used to memorize and evaluate.\n",
    "    :param learning_rate (scalar): learning rate use in the gradient descent algorithm.\n",
    "    :param LOSS_FCT (tensor): function used to measure the loss of the model we want to minimize.\n",
    "    :param VARIABLE_LIST ([tf.Variables]): list of variables w.r.t. which the gradient is computed.\n",
    "    :param feed_dict (dictionary): dictionary of {placeholder : numpy array}.\n",
    "    :param precision (scalar): desired precision.\n",
    "    :param MAX_STEPS (int): maximum number of steps allowed to prevent an infinite loop.\n",
    "    :returns ((list, int, list, list)):\n",
    "        - First element (list of arrays or scalars): optimal values found for the variables of the model.\n",
    "        - Second element (int): number of steps needed to converge.\n",
    "        - Third element (list of lists of arrays or scalars): recorded values for the variables of the model.\n",
    "        - Fourth element (list of scalars): recorded values for the loss function.\n",
    "    \"\"\"\n",
    "    #    \n",
    "    #\n",
    "    #    Your code here\n",
    "    #\n",
    "    \n",
    "    #return optimum_variable_values, n_steps, trajectory, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what the new algorithm gives (warning, this can take a few minutes to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict={X: np.atleast_2d(x).T, Y: np.atleast_2d(y).T}\n",
    "\n",
    "(a_opt, b_opt), n_steps, trajectory, losses = gradient_descent(sess, 1e-4,\n",
    "                                                               LOSS, [A, B],\n",
    "                                                               feed_dict)\n",
    "sess.close()\n",
    "\n",
    "print('Optimum found (%.3f, %.3f) in %d steps of Gradient Descent.' % (a_opt, b_opt, n_steps))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot([param[0] for param in trajectory], [param[1] for param in trajectory])\n",
    "plt.title('Trajectory')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving new machine learning problems quickly\n",
    "\n",
    "One of the key perks of automated differentiation is that we can solve any other optimization problem as long as we are able to instantiate a loss function for it.\n",
    "\n",
    "Let's take for instance a classification data set and solve the corresponding problem with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 1024\n",
    "x = np.random.normal(0.0, 0.5, n_samples)\n",
    "pr = 1.0 / (1.0 + np.exp(-5.5 * x + 1.5))\n",
    "\n",
    "y = np.random.binomial(1, pr, n_samples)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Regressor')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a new model\n",
    "\n",
    "To build a new model predicting Y based on X, we first build a logit layer trying to capture the linear dependency x -> -5.5 * x + 1.5.\n",
    "\n",
    "We will choose to encode each label as a single integer and not as a one_hot vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take care of the data and prepare placeholders for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create a placeholder for the variable X\n",
    "#\n",
    "#    Your code here\n",
    "#\n",
    "\n",
    "## Create a placeholder for the variable Y\n",
    "#\n",
    "#    Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, start building a logit model based on X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create variables that create a linear model with offset with X as a regressor.\n",
    "#\n",
    "#   Your code here\n",
    "#\n",
    "\n",
    "LOGIT = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, instantiate a loss that will compute a cross-entropy loss on top of a softmax layer.\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "https://www.tensorflow.org/api_docs/python/tf/reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a LOSS function linking the actual labels in Y to the logit layer\n",
    "#\n",
    "# Your code here\n",
    "#\n",
    "\n",
    "LOSS = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the gradient descent operates on the loss (and for now we don't split the data in train / test sets), the cross entropy loss score is not very readible. It is generally better to check at the same time the accuracy of the classifier based on the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an ACCURACY tensor\n",
    "#\n",
    "# Your code here\n",
    "#\n",
    "\n",
    "ACCURACY = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets slighy modify our gradient function so as to keep track of the values for the accuarcy.\n",
    "To simplify the stopping criterion we now stop after MAX_STEPS steps of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_with_eval(tf_session, learning_rate, \n",
    "                               LOSS_FCT, VARIABLE_LIST, EVAL_FCT,\n",
    "                               feed_dict,\n",
    "                               MAX_STEPS=200):\n",
    "    \"\"\"\n",
    "    Gradient descent using tensorflow to compute the gradient of LOSS_FCT w.r.t the variables in VARIABLE_LIST.\n",
    "    Here we are using all the data set. Feed_dict remains constant.\n",
    "    :param tf_session (Tensorflow session): session that will be used to memorize and evaluate.\n",
    "    :param learning_rate (scalar): learning rate use in the gradient descent algorithm.\n",
    "    :param LOSS_FCT (tensor): function used to measure the loss of the model we want to minimize.\n",
    "    :param VARIABLE_LIST ([tf.Variables]): list of variables w.r.t. which the gradient is computed.\n",
    "    :param EVAL_FCT (tensor): function used to measure the performance of the model we want to minimize.\n",
    "    :param feed_dict (dictionary): dictionary of {placeholder : numpy array}.\n",
    "    :param MAX_STEPS (int): maximum number of steps allowed to prevent an infinite loop.\n",
    "    :returns ((list, list, list, list)):\n",
    "        - First element (list of arrays or scalars): optimal values found for the variables of the model.\n",
    "        - Second element (list of lists of arrays or scalars): recorded values for the variables of the model.\n",
    "        - Third element (list of scalars): recorded values for the loss function.\n",
    "        - Fourth element (list of scalars): recorded values for the evaluation function.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #    Your code here\n",
    "    #\n",
    "    \n",
    "    #return optimum_variable_values, trajectory, losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run it on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "feed_dict={X: np.atleast_2d(x).T, Y: np.asarray(y, dtype=np.int32)}\n",
    "\n",
    "_, trajectory, losses, accuracies = gradient_descent_with_eval(sess, 1e-1,\n",
    "                                                               LOSS, [A, B], ACCURACY,\n",
    "                                                               feed_dict)\n",
    "sess.close()\n",
    "\n",
    "print('Optimum found in %d steps of Gradient Descent.' % n_steps)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS HERE\n",
    "\n",
    "a) What do notice about the run-time?\n",
    "\n",
    "### Solution:\n",
    "\n",
    "a) It's slow... and we waste a lot of time computing early iterations for which we do not really need a precise estimate of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated differentiation for Stochastic Gradient Descent\n",
    "\n",
    "Although it makes our life slightly easier because we do not have to compute gradients, automated differentiation has another perk: it can compute the gradient of the loss with respect to the model parameters as we choose different parts of our dataset to compute the loss.\n",
    "\n",
    "Namely, we can now directly change what we assign in the placholders by controlling our feed_dict and run SGD for a certain batch_size (the number of elements we consider simultanously).\n",
    "\n",
    "SGD is particularly helpful as it can speed up the training as we do not waste the time taken by looking at all the dataset in the early steps of the descent where our parameters are rubish anyway.\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function that returns batch_size samples of (x, y) pairs chosen from the logistic regression data set.\n",
    "# Don't forget to shuffle your data first.\n",
    "#\n",
    "# Your code here for the shuffling\n",
    "#\n",
    "\n",
    "permutation = \n",
    "\n",
    "def get_next_batch(batch_size, offset):\n",
    "    \"\"\"\n",
    "    Collect batch_size samples from the data set starting at offset.\n",
    "    Wrap if necessary.\n",
    "    :param offset (int): where the collection of samples starts in the dataset.\n",
    "    :param batch_size (int): how many samples are collected.\n",
    "    :return ((array like, array like)):\n",
    "        - First element (numpy array like): batch of values for x.\n",
    "        - Second element (numpy array like): batch of values for y corresponding to those in the batch for x.\n",
    "    \"\"\"\n",
    "    selection = np.arange(offset, offset + batch_size) % n_samples\n",
    "    return x[selection], y[selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all the ingredients we need to code SGD on top of tensorflow.\n",
    "\n",
    "In this tutorial we will only evaluate the accuracy on the batch we are computing the gradient with.\n",
    "\n",
    "However, it is better to evaluate the accuracy on an independent test set that comprises of many more samples than there are in a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(tf_session, learning_rate, batch_size, \n",
    "                                LOSS_FCT, VARIABLE_LIST, EVAL_FCT,\n",
    "                                next_batch_fct,\n",
    "                                MAX_STEPS=200):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent using tensorflow to compute the gradient of LOSS_FCT \n",
    "    w.r.t the variables in VARIABLE_LIST on sub-samples of the data set.\n",
    "    Here we are using all the data set. Feed_dict remains constant.\n",
    "    :param tf_session (Tensorflow session): session that will be used to memorize and evaluate.\n",
    "    :param learning_rate (scalar): learning rate use in the gradient descent algorithm.\n",
    "    :param batch_size (int): number of elements sampled from the data set at each step.\n",
    "    :param LOSS_FCT (tensor): function used to measure the loss of the model we want to minimize.\n",
    "    :param VARIABLE_LIST ([tf.Variables]): list of variables w.r.t. which the gradient is computed.\n",
    "    :param EVAL_FCT (tensor): function used to measure the performance of the model we want to minimize.\n",
    "    :param next_batch_fct ((int, int) -> (array, array)): collect samples for the next training step.\n",
    "    :param MAX_STEPS (int): maximum number of steps allowed to prevent an infinite loop.\n",
    "    :returns ((list, list, list, list)):\n",
    "        - First element (list of arrays or scalars): optimal values found for the variables of the model.\n",
    "        - Second element (list of lists of arrays or scalars): recorded values for the variables of the model.\n",
    "        - Third element (list of scalars): recorded values for the loss function.\n",
    "        - Fourth element (list of scalars): recorded values for the evaluation function.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #    Your code here\n",
    "    #\n",
    "    \n",
    "    #return optimum_variable_values, trajectory, losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can run this algorithm to solve our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "_, trajectory, losses, accuracies = stochastic_gradient_descent(sess, 1e-1, 8,\n",
    "                                                                LOSS, [A, B], ACCURACY,\n",
    "                                                                get_next_batch)\n",
    "sess.close()\n",
    "\n",
    "print('Optimum found in %d steps of Gradient Descent.' % n_steps)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, this should run faster now, each iteration being faster.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "a) Do we actually achieve faster training systematically?\n",
    "\n",
    "b) We used a constant learning rate here, is this a good idea?\n",
    "\n",
    "Answers:\n",
    "\n",
    "a)\n",
    "\n",
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
