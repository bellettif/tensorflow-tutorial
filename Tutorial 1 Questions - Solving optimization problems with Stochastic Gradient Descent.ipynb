{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving problems with Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part of the assignmentment is to get familiar with stochastic gradient descent. Specifically, we'll be looking at trying to predict the output of a certain function\n",
    "```\n",
    "y = f(x)\n",
    "```\n",
    "Our goal is to predict the output of `f` given input `x`. To learn, we'll be given a bunch of input-output examples,\n",
    "```\n",
    "(x_1, f(x_1))\n",
    "(x_2, f(x_2))\n",
    "...\n",
    "(x_N, f(x_N))\n",
    "\n",
    "```\n",
    "In this case, `f(x) = x*x` is a simple quadratic function, but the techniques we develop here are broadly applicable to functions that are much more complicated and whose analytic formula may be unknown or intractable to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by building `f(x) = x*x` function in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(array_in):\n",
    "    \"\"\"\n",
    "    Computes the element-wise square of a scalar or array.\n",
    "    :param array_in (numpy array like): The argument to compute the square of.\n",
    "    :return (numpy array like): Element-wise squared input.\n",
    "    \"\"\"\n",
    "    return np.power(array_in, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll generate some random input-output data for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 1024\n",
    "x = np.random.normal(0.0, 0.5, n_samples)\n",
    "y = 2.5 * x + 2.25 + np.random.normal(0.0, 0.05, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the goal is to learn a function that predicts `y` given `x`, assuming that we don't know the true function `f`. The first thing that we'll try a linear model. So we'll predict `y` according the equation `y_pred = ax + b`. This model has two parameters, `a` and `b`.\n",
    "\n",
    "Now how should we measure how \"good\" our predictions of `y` are? A common approach is to use the squared error, which simply squares the difference between the true `y` value and our predicted `y` value. We define our _loss_ as the average of these values, called the mean squared error (AKA least squares loss).\n",
    "\n",
    "All together, the following code computes the least squares loss for simple two-parameter, linear model on our generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(a, b):\n",
    "    \"\"\"\n",
    "    Definition of the mean squared error function.\n",
    "    :param a (scalar): Slope of the linear model.\n",
    "    :param b (scalar): Offset of the linear model.\n",
    "    :return (scalar): Mean squared error.\n",
    "    \"\"\"\n",
    "    y_pred = a * x + b\n",
    "    error = y - y_pred\n",
    "    return np.mean(square(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to see how good our model is for a specific set of `a` and `b` values, we can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss(0.01, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to decrease this loss as much as possible. Let's visualize how different `a` and `b` values affect the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "a_grid = np.arange(0.0, 5.0, 0.05)\n",
    "b_grid = np.arange(0.0, 5.0, 0.05)\n",
    "error_plot = np.asanyarray([[loss(a, b) \n",
    "                             for b in b_grid]\n",
    "                            for a in a_grid])\n",
    "A, B = np.meshgrid(a_grid, b_grid)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "im = ax.plot_surface(A, B, error_plot, cmap=cm.coolwarm, linewidth=0)\n",
    "\n",
    "plt.title('Empirical error as a function of a and b')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At any point in this 2D space, we can compute the sensitivity of the error with respect to the parameters a and b. This is called the gradient and is defined as the vector [dloss / da, dloss / db]. You can think of the gradient as a generalization of derivatives to more than one input dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the analytical expression for loss(a,b) and derive the corresponding gradient:\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "Write a function to compute the gradient as a pair of outpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_grad(a, b):\n",
    "    \"\"\"\n",
    "    Gradient of the mean squared error loss function.\n",
    "    :param a (scalar): Slope of the linear model.\n",
    "    :param b (scalar): Offset of the linear model.\n",
    "    :return (tuple):\n",
    "        - first element (scalar): Derivative of the loss function w.r.t a.\n",
    "        - second element (scalar): Derivative of the loss function w.r.t. b.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #  Your code here\n",
    "    #\n",
    "\n",
    "    #return (something, something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The minizing (a, b) pair is the lowest point we can find on the 3d surface above. The most direct way of getting to the point is to follow a stepest descent approach.\n",
    "\n",
    "For a continuously differentiable function, the direction of steepest descent at a point is exactly given by the gradient at then point. (Remember that the gradient is a function of the input, i.e. `a` and `b` in our example).\n",
    "\n",
    "Gradient descent works as follows:\n",
    "1. Start a point `(a[0], b[0])`\n",
    "2. Compute the gradient of the loss function at the point: `(g[0], g[1])`\n",
    "3. Create a new point `(a[1] = a[0] - epsilon * g_0, b[1] = b[0] - epsilon * g_1)`\n",
    "4. Repeat steps 1-3 until some stopping criterion\n",
    "\n",
    "Here, epsilon is our learning rate. Note that `g[0]` is the partial derivative of the loss function with respect to its first input and `g[1]` its partial derivative with respect to its second input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE\n",
    "Update the old parameters a and b with a pre-computed gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_params(a, b, learning_rate, gradient):\n",
    "    \"\"\"\n",
    "    Return updated values for a and b following the opposite direction to the gradient.\n",
    "    :param a (scalar): slope of the linear model.\n",
    "    :param b (scalar): offset of the linear model.\n",
    "    :learning_rate (scalar): gradient multiplyer in the update.\n",
    "    :gradient ((scalar, scalar)): computed gradient.\n",
    "    :return (tuple):\n",
    "        - first element (scalar): Updated slope.\n",
    "        - second element (scalar): Updated offset.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #   Your code here\n",
    "    #\n",
    "\n",
    "    #return (something, something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE\n",
    "\n",
    "Code the gradient descent algorithm starting a parameter value (a_init, b_init).\n",
    "\n",
    "The stopping criterion is squared_l2_norm(gradient) < precision * (1 + abs(loss)) or when `MAX_STEPS` have been taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(a_init, b_init, learning_rate, \n",
    "                     loss_eval_fct, gradient_eval_fct, \n",
    "                     precision=1e-4, MAX_STEPS=10000):\n",
    "    \"\"\"\n",
    "    :param a_init (scalar): Initial value of `a`.\n",
    "    :param b_init (scalar): Initial value of `b`.\n",
    "    :param learning_rate (scalar): Learning rate for gradient descent.\n",
    "    :param loss_eval_fct ((scalar, scalar) -> scalar). Give `x`, return `f(x)`\n",
    "    :param gradient_eval_fct ((scalar, scalar) -> (scalar, scalar)): Give `x`, return the gradient of `f` at `x`.\n",
    "    :param precision (scalar): precision desired.\n",
    "    :param MAX_STEPS (int): number of steps before the descent is stopped to avoid an infinite loop.\n",
    "    :return: A tuple\n",
    "        - First element (scalar): final value for a.\n",
    "        - Second element (scalar): final value for b.\n",
    "        - Third element (int): number of steps used to converge.\n",
    "        - Fourth element (list): trajectory, a list where `trajectory[i]` is a tuple of the (a, b) values at step `i`.\n",
    "        - Firth element (list): losses, a list where `losses[i]` is the loss at step `i`.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #    Your code here\n",
    "    #\n",
    "    \n",
    "    #return (a_opt, b_opt, n_steps, trajectory, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the gradient descent algorithm to our initial least squares problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_opt, b_opt, n_steps, trajectory, losses = gradient_descent(0.0, 0.0, 2e-1, loss, loss_grad)\n",
    "\n",
    "print('Optimum found (%.3f, %.3f) in %d steps of Gradient Descent.' % (a_opt, b_opt, n_steps))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot([param[0] for param in trajectory], [param[1] for param in trajectory])\n",
    "plt.title('Trajectory')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the sensitivity of the run time with respect to your choice of initial values?\n",
    "\n",
    "b) What is the sensitivity of the run time with respect to the precision?\n",
    "\n",
    "c) What is the sensitivity of the run time with respect to the learning rate? \n",
    "\n",
    "d) What happens if the learning rate is very high? What happens if it's very low?\n",
    "\n",
    "### YOUR ANSWERS HERE:\n",
    "\n",
    "a)\n",
    "\n",
    "b)\n",
    "\n",
    "c)\n",
    "\n",
    "d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Considering a random gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of computing our gradient holistically, let us only focus on a few (x, y) pairs at the time.\n",
    "\n",
    "The loss function we consider here is an empirical average obtained from hypothetically independent identically distributed samples. By linearity of the gradient, the gradient function here is also an average as well.\n",
    "\n",
    "Using the Law of Large Numbers we can approximate this sum by a few samples randomly selected. This smaller collection of samples is called a batch (or \"minibatch\") and the number of samples we choose to represent our data set at each of step of the gradient descent.\n",
    "\n",
    "The key insight here is that for the first steps of the gradient descent it is not very informative to carefully examine the value of the contributions to the gradient from each sample. We are most likely far from the optimum we seek anyway and therefore an imperfect vector indicating an approximation of the steepest descent direct is enough to get us closer to the optimum.\n",
    "\n",
    "We just follow Gandalf's recommendation in the Moria mine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_img_from_url(url):\n",
    "    from PIL import Image\n",
    "    import cStringIO\n",
    "    import urllib2\n",
    "    im = Image.open(cStringIO.StringIO(urllib2.urlopen(url).read()))\n",
    "    plt.imshow(im, origin='upper')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plot_img_from_url('http://data.whicdn.com/images/53685638/large.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE\n",
    "\n",
    "Write a random approximation of the gradient obtained by only selecting batch_size distinct samples uniformly at randnom.\n",
    "\n",
    "First, we shuffle the data set which can help enforce the iid assumption about our samples with datasets that may have similar data points clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = np.random.choice(np.arange(n_samples), n_samples)\n",
    "\n",
    "x = x[permutation]\n",
    "y = y[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function that selects batch_size distinct samples of corresponding x, y observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_samples(offset, batch_size):\n",
    "    \"\"\"\n",
    "    Collect batch_size samples from the data set starting at offset.\n",
    "    Wrap if necessary.\n",
    "    :param offset (int): where the collection of samples starts in the dataset.\n",
    "    :param batch_size (int): how many samples are collected.\n",
    "    :return ((array like, array like)):\n",
    "        - First element (numpy array like): batch of values for x.\n",
    "        - Second element (numpy array like): batch of values for y corresponding to those in the batch for x.\n",
    "    \"\"\"\n",
    "    selection = np.arange(offset, offset + batch_size) % n_samples\n",
    "    return x[selection], y[selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a similar function that estimates the gradient for a pair of parameter values as an empirical average over the elements of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sampled_loss_grad(x_batch, y_batch, a, b):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error for the collection of samples collected from the data set.\n",
    "    :param x_batch (numpy array like): collection of values for x.\n",
    "    :param y_batch (numpy array like): collection of values for y corresponding to the values for x in x_batch.\n",
    "    :param a (scalar): slope of the linear model.\n",
    "    :param b (scalar): offset of the linear model.\n",
    "    :return (tuple):\n",
    "        - First element (scalar): derivative of the mean squared error on the sub-set of samples w.r.t. a.\n",
    "        - Second element (scalar): derivative of the mean squared error on the sub-set of samples w.r.t. b.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #     Your code here\n",
    "    #\n",
    "    \n",
    "    # return pair of scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "We now re-implement the gradient descent algorithm but with two major modifications:\n",
    "1. We use an approximate evaluation of the gradient of the loss function based on sample collections of length batch_size.\n",
    "2. We use the loss evaluated over the entire dataset as a stopping criterion, but we only compute it every evaluate_loss_every iterations. We stop when don't improve the global loss more than the prescribed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(a_init, b_init, learning_rate, \n",
    "                                loss_eval_fct, sampled_gradient_eval_fct,\n",
    "                                batch_size=8,\n",
    "                                evaluate_loss_every=4, precision=1e-5, MAX_STEPS=10000):\n",
    "    \"\"\"\n",
    "    :param a_init (scalar): Initial value of `a`.\n",
    "    :param b_init (scalar): Initial value of `b`.\n",
    "    :param learning_rate (scalar): Learning rate for gradient descent.\n",
    "    :param loss_eval_fct ((scalar, scalar) -> scalar). Give `a,b`, return `f(a,b)`\n",
    "    :param sampled_gradient_eval_fct ((array, array, scalar, scalar) -> (scalar, scalar)): Approx. grad. of `f`.\n",
    "    :param batch_size (int): number of elements used to compute the sampled gradient.\n",
    "    :param evaluate_loss_every (int): only evaluate on the data set evaluate_loss_every steps.\n",
    "    :param precision (scalar): precision desired.\n",
    "    :param MAX_STEPS (int): number of steps before the descent is stoped to avoid an infinite loop.\n",
    "    :return: A tuple\n",
    "        - First element (scalar): final value for a.\n",
    "        - Second element (scalar): final value for b.\n",
    "        - Third element (int): number of steps used to converge.\n",
    "        - Fourth element (list): trajectory, a list where `trajectory[i]` is a tuple of the (a, b) values at step `i`.\n",
    "        - Firth element (list): losses, a list where `losses[i]` is the loss at step `i`.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #    Your code here\n",
    "    #\n",
    "    \n",
    "    #return (a_opt, b_opt, n_steps, trajectory, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the stochastic gradient descent algorithm to our initial least squares problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_opt, b_opt, n_steps, trajectory, losses = stochastic_gradient_descent(0.0, 0.0, 2e-1, \n",
    "                                                                        loss, sampled_loss_grad)\n",
    "\n",
    "print('Optimum found (%.3f, %.3f) in %d steps of Stochatic gradient Descent.' % (a_opt, b_opt, n_steps))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot([param[0] for param in trajectory], [param[1] for param in trajectory])\n",
    "plt.title('Trajectory')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compared to Gradient Descent, do we compute more or less iterations?\n",
    "\n",
    "b) Compared to Gradient Descent, do we increase or decrease the run time?\n",
    "\n",
    "c) What is the impact of the batch_size parameter on the smoothness of the trajectory? On the number of steps required to converge?\n",
    "\n",
    "d) Give multiple tries to the algorithm, does the loss always decrease?\n",
    "\n",
    "e) What is is the influence of the learning rate parameter?\n",
    "\n",
    "### YOUR ANSWERS HERE:\n",
    "\n",
    "a)\n",
    "\n",
    "b)\n",
    "\n",
    "c)\n",
    "\n",
    "d)\n",
    "\n",
    "e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decaying learning rate\n",
    "\n",
    "A constant learning rate is enough to get close to the optimum rapidly. However at the end of the descent we get bouncing around the optimum without being able to get as close as want to it.\n",
    "\n",
    "To be able to achieve this, we want a series of learning rates that sums to infinity and whose sum of squares is finite.\n",
    "\n",
    "Let's use alpha/(1 + t) as a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_decaying_rate(a_init, b_init, initial_learning_rate, \n",
    "                                              loss_eval_fct, sampled_gradient_eval_fct,\n",
    "                                              batch_size=8,\n",
    "                                              evaluate_loss_every=4, precision=1e-5, MAX_STEPS=10000):\n",
    "    \"\"\"\n",
    "    :param a_init (scalar): Initial value of `a`.\n",
    "    :param b_init (scalar): Initial value of `b`.\n",
    "    :param initial_learning_rate (scalar): Initial learning rate for gradient descent.\n",
    "    :param loss_eval_fct ((scalar, scalar) -> scalar). Give `a,b`, return `f(a,b)`\n",
    "    :param sampled_gradient_eval_fct ((array, array, scalar, scalar) -> (scalar, scalar)): Approx. grad. of `f`.\n",
    "    :param batch_size (int): number of elements used to compute the sampled gradient.\n",
    "    :param evaluate_loss_every (int): only evaluate on the data set evaluate_loss_every steps.\n",
    "    :param precision (scalar): precision desired.\n",
    "    :param MAX_STEPS (int): number of steps before the descent is stoped to avoid an infinite loop.\n",
    "    :return: A tuple\n",
    "        - First element (scalar): final value for a.\n",
    "        - Second element (scalar): final value for b.\n",
    "        - Third element (int): number of steps used to converge.\n",
    "        - Fourth element (list): trajectory, a list where `trajectory[i]` is a tuple of the (a, b) values at step `i`.\n",
    "        - Firth element (list): losses, a list where `losses[i]` is the loss at step `i`.\n",
    "    \"\"\"\n",
    "    #\n",
    "    #    Your code here\n",
    "    #\n",
    "    \n",
    "    #return (a_opt, b_opt, n_steps, trajectory, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_opt, b_opt, n_steps, trajectory, losses = stochastic_gradient_descent_decaying_rate(0.0, 0.0, 0.1, \n",
    "                                                                                      loss, sampled_loss_grad)\n",
    "\n",
    "print('Optimum found (%.3f, %.3f) in %d steps of Stochatic gradient Descent.' % (a_opt, b_opt, n_steps))\n",
    "\n",
    "plt.subplot(121)\n",
    "trajectory=trajectory\n",
    "colors = np.linspace(0.0, 1.0, len(trajectory))\n",
    "plt.scatter([param[0] for param in trajectory], [param[1] for param in trajectory],\n",
    "            c=colors, cmap='plasma', edgecolors='none')\n",
    "plt.title('Trajectory')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Why do we want our sum of steps to be infinite in size if an infinite number of steps occur?\n",
    "\n",
    "b) Why do we want our sum of squared steps to be infinite in size if an infinite number of steps occur?\n",
    "\n",
    "### YOUR ANSWERS HERE:\n",
    "\n",
    "a)\n",
    "\n",
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
